<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Posts on Human and Machine Learning in Cognitive Science</title><link>tylerjamesmalloy.github.io/posts/</link><description>Recent content in Posts on Human and Machine Learning in Cognitive Science</description><generator>Hugo -- gohugo.io</generator><language>en</language><copyright>&lt;a href="https://creativecommons.org/licenses/by-nc/4.0/" target="_blank" rel="noopener">CC BY-NC 4.0&lt;/a></copyright><lastBuildDate>Mon, 17 Jul 2023 00:00:00 +0000</lastBuildDate><atom:link href="tylerjamesmalloy.github.io/posts/index.xml" rel="self" type="application/rss+xml"/><item><title>IEEE S&amp;P 2023</title><link>tylerjamesmalloy.github.io/posts/ieee-snp-2023/</link><pubDate>Mon, 17 Jul 2023 00:00:00 +0000</pubDate><guid>tylerjamesmalloy.github.io/posts/ieee-snp-2023/</guid><description>Keynote: Bart Preneel (KU Leuven) Discussion on the goals and realities of privacy policy implementations with examples from history of the 3 major &amp;lsquo;privacy wars&amp;rsquo;, their causes and impacts on public policy of privacy and security.
Session: Phishing/fraud/scams Forward Pass: On the Security Implications of Email Forwarding Mechanism and Policy
Enze Liu (University of California, San Diego), Gautam Akiwate (Stanford University), Mattijs Jonker (University of Twente), Ariana Mirian (University of California, San Diego), Grant Ho (University of California, San Diego), Geoffrey M.</description><content type="html"><![CDATA[<h2 id="keynote-bart-preneel-ku-leuven">Keynote: Bart Preneel (KU Leuven)</h2>
<p>Discussion on the goals and realities of privacy policy implementations with examples from history of the 3 major &lsquo;privacy wars&rsquo;, their causes and impacts on public policy of privacy and security.</p>
<h2 id="session-phishingfraudscams">Session: Phishing/fraud/scams</h2>
<p><strong><a href="https://arxiv.org/abs/2302.07287">Forward Pass: On the Security Implications of Email Forwarding Mechanism and Policy</a><a href="https://www.ieee-security.org/TC/EuroSP2023/program.html#eurosp2017-71"></a></strong><br>
<strong>Enze Liu</strong> (University of California, San Diego), <strong>Gautam Akiwate</strong> (Stanford University), <strong>Mattijs Jonker</strong> (University of Twente), <strong>Ariana Mirian</strong> (University of California, San Diego), <strong>Grant Ho</strong> (University of California, San Diego), <strong>Geoffrey M. Voelker</strong> (University of California, San Diego), <strong>Stefan Savage</strong> (University of California, San Diego)</p>
<p>Email forwarding impacts the effectiveness of security that is designed to ensure that emails are sent from the appropriate addresses like SPF. Also currently SPF is sourced through aggregated lists like through outlook.</p>
<ul>
<li>Open forwarding, not asking if you own the email you are sending the forwarded emails to.</li>
<li>Whitelisting: Spam filters edited to allow emails from some domains. Can be issues with forwarding if you spoof an email as from a whitelisted domain, and this can be combined with email forwarding to forward arbitrary domains.</li>
</ul>
<p>Attack description: Whitelist state.gov in your outlook, spoof an email as coming from state.gov and send it to yourself, open forward message to target email to pass SPF.</p>
<p><strong><a href="https://s3.eurecom.fr/post/2023/03/02/android-notify-me-when-it-is-time-to-go-phishing/">Android, notify me when it is time to go phishing</a><a href="https://www.ieee-security.org/TC/EuroSP2023/program.html#eurosp2017-2"></a></strong><br>
<strong>Antonio Ruggia</strong> (University of Genoa), <strong>Andrea Possemato</strong> (EURECOM), <strong>Alessio Merlo</strong> (University of Genoa), <strong>Dario Nisi</strong> (EURECOM), <strong>Simone Aonzo</strong> (EURECOM)
Exploit Inotify for state inference, specifically to tell when the target application has been started, so that a malicious version of the application can be simultaneously launched that will collect user information.</p>
<p><strong><a href="https://arxiv.org/abs/2210.15043">Active Countermeasures for Email Fraud</a><a href="https://www.ieee-security.org/TC/EuroSP2023/program.html#eurosp2017-5"></a></strong><br>
<strong>Wentao Chen</strong> (University of Bristol), <strong>Fuzhou Wang</strong> (City University of Hong Kong), <strong>Matthew Edwards</strong> (University of Bristol)</p>
<p>Social engineering active defense (SEAD): degrade the ability of the attacker to victimize, through the use of social engineering, like scam-baiting (scam scammers, humiliate scammers, or waste their time). Issue is that it requires volunteers, much smaller community than the target population, doesn&rsquo;t scale.</p>
<p>Automatic scam-baiting</p>
<ul>
<li>Trained on open-sourced scam baiter data.</li>
<li>GPT-neo compared against templates statements.</li>
<li>Concurrent engagement of scammers and these fake systems.</li>
<li>Chat-GPT performs very well.</li>
</ul>
<p>Question: Would it be useful to detect if the scammer&rsquo;s conversation is being generated by a different automatic chatter?</p>
<h2 id="session-crypto--formal-methods-i">Session: Crypto + formal methods I</h2>
<p><strong><a href="https://arxiv.org/abs/2306.08169">Multi-Factor Credential Hashing for Asymmetric Brute-Force Attack Resistance</a><a href="https://www.ieee-security.org/TC/EuroSP2023/program.html#eurosp2017-6"></a></strong><br>
<strong>Vivek Nair</strong> (UC Berkeley), <strong>Dawn Song</strong> (UC Berkeley)</p>
<p><a href="https://mfkdf.com/">MFKDF website</a></p>
<p><a href="https://hashmob.net/">Hashmob</a>: Competition for cracking passwords after data breaches, occurs within 12-15 months after breach, salted passwords have lower rates of cracking, and adaptive hashing functions have lower rates too.</p>
<p>Bcrypt and other adaptive hashing functions. Issue is that there is a symmetric resistance requiring more time/resources from the user.</p>
<p>MFA is also used in a different factor/storage. MFCHF proposes adding the MFA information to the hash. Requires incorporating TOTP codes into static hashes.</p>
<p><strong><a href="https://eprint.iacr.org/2021/1603">CHEX-MIX: Combining Homomorphic Encryption with Trusted Execution Environments for Oblivious Inference in the Cloud</a><a href="https://www.ieee-security.org/TC/EuroSP2023/program.html#eurosp2017-7"></a></strong><br>
<strong>Deepika Natarajan</strong> (University of Michigan-Ann Arbor), <strong>Andrew Loveless</strong> (University of Michigan-Ann Arbor), <strong>Wei Dai</strong> (Microsoft Research), <strong>Ron Dreslinski</strong> (University of Michigan-Ann Arbor)</p>
<p>Insecurity in cloud computing with ML applications like voice assistants. Possible to reverse engineer model parameters through insecurities. Can also use details of model to de-anonymize user data.</p>
<p>Adversary model assumes a rational model provider and a malicious client and cloud service provider. Goal is cloud oblivious inference. Homeomorphic Encryption doesn&rsquo;t achieve this because in enables data privacy but clients can edit inputs maliciously. TEEs like SGX enclaves don&rsquo;t work, attestation requires checking the ML code for vulnerabilities.</p>
<p><strong>A Generic Obfuscation Framework for Preventing ML-Attacks on Strong-PUFs through Exploitation of DRAM-PUFs<a href="https://www.ieee-security.org/TC/EuroSP2023/program.html#eurosp2017-8"></a></strong><br>
<strong>Owen Millwood</strong> (University of Sheffield), <strong>Meltem Kurt Pehlivanoğlu</strong> (Kocaeli University), <strong>Jack Miskelly</strong> (Queen&rsquo;s University Belfast), <strong>Aryan Mohammadi Pasikhani</strong> (University of Sheffield), <strong>Prosanta Gope</strong> (University of Sheffield), <strong>Elif Bilge Kavun</strong> (University of Passau)</p>
<p>Physically unclonable functions should be unique and secure. ML attacks try to predict new challenge response pairs to compromise security. Obfuscating techniques prevent ML-MA but are too PUF design specific and attack countermeasures are too costly. Solution could be to supplement cryptography with PUFs.</p>
<p><strong><a href="https://arxiv.org/abs/2303.04500">Automatic verification of transparency protocols</a><a href="https://www.ieee-security.org/TC/EuroSP2023/program.html#eurosp2017-9"></a></strong><br>
<strong>Vincent Cheval</strong> (INRIA Paris, France), <strong>José Moreira</strong> (Valory AG, Switzerland), <strong>Mark Ryan</strong> (University of Birmingham)</p>
<p>Transparency protocols allow for actions to be publicly monitored by observers. Examples are transparent decryption which prevents secretly decrypting some encrypted information. ProVerif is an example that enables this, verification of ledger&rsquo;s proofs. This is done by adding Lemmas/Axioms/Restrictions to ledger verifications.</p>
<h2 id="session-security-and-ai">Session: Security and AI</h2>
<p><strong>Protecting Voice-Controllable Devices Against Self-Issued Voice Commands<a href="https://www.ieee-security.org/TC/EuroSP2023/program.html#eurosp2017-13"></a></strong><br>
<strong>Sergio Esposito</strong> (Royal Holloway University of London), <strong>Daniele Sgandurra</strong> (Royal Holloway University of London), <strong>Giampaolo Bella</strong> (Università degli Studi di Catania)</p>
<p>Electronically generated voices should be able to use smart devices for disabled users. Current methods can&rsquo;t tell if a voice command came from the same device. Solution is to use a twin ANN to compare recorded and original audios.</p>
<p><strong><a href="https://arxiv.org/pdf/2112.02918.pdf">When the Curious Abandon Honesty: Federated Learning Is Not Private</a><a href="https://www.ieee-security.org/TC/EuroSP2023/program.html#eurosp2017-14"></a></strong><br>
<strong>Franziska Boenisch</strong> (Vector Institute), <strong>Adam Dziedzic</strong> (University of Toronto and Vector Institute), <strong>Roei Schuster</strong> (Vector Institute), <strong>Ali Shahin Shamsabadi</strong> (Vector Institute and The Alan Turing Institute), <strong>Ilia Shumailov</strong> (Vector Institute), <strong>Nicolas Papernot</strong> (University of Toronto and Vector Institute)</p>
<p>Active attack of federated learning shared ML models with trap weights which induce the case where most input data has 0 gradient due to the RELU function 0ing out negative numbers. Just passively/honest querying can cause ~5% of neurons to be only activated by 1 data point, which allows for extraction. This means more neurons and smaller mini batches allow for more extraction. Some solutions are to add noisy data to training or testing.</p>
<p><strong><a href="https://arxiv.org/abs/2208.10605">SoK: Explainable Machine Learning for Computer Security Applications</a><a href="https://www.ieee-security.org/TC/EuroSP2023/program.html#eurosp2017-16"></a></strong><br>
<strong>Azqa Nadeem</strong> (Delft University of Technology), <strong>Daniël Vos</strong> (Delft University of Technology), <strong>Clinton Cao</strong> (Delft University of Technology), <strong>Luca Pajola</strong> (University of Padua), <strong>Simon Dieck</strong> (Delft University of Technology), <strong>Robert Baumgartner</strong> (Delft University of Technology), <strong>Sicco Verwer</strong> (Delft University of Technology)</p>
<p>XAI within Cybersecurity is broken into the model user, model designer, and adversary. Explanations should be tailored to who the explanation is for, user/designer. Explanation systems can be manipulated by adversaries to produce false explanations, or utilized by them to aid in the exploitation of AI services.</p>
<p>User studies of XAI typically doesn&rsquo;t include model users, and doesn&rsquo;t treat explanation understandably as a target for XAI. Some issues where explanations can violate privacy. Gradient Boosting Machine trained on Netflow data, more interpretable models can be used in well defined cases without negative impacts to performance.</p>
<h2 id="session-side-channels-and-transient-execution">Session: Side Channels and Transient Execution</h2>
<p><strong><a href="https://arxiv.org/abs/2212.10221">SoK: Analysis of Root Causes and Defense Strategies for Attacks on Microarchitectural Optimizations</a><a href="https://www.ieee-security.org/TC/EuroSP2023/program.html#eurosp2017-34"></a></strong><br>
<strong>Nadja Ramhöj Holtryd</strong> (Chalmers University of Technology), <strong>Madhavan Manivannan</strong> (Chalmers University of Technology), <strong>Per Stenström</strong> (Chalmers University of Technology)</p>
<p>What are the similarities and differences across optimization attacks, and are there common root causes for timing based attacks and side-channel attacks.</p>
<ul>
<li>Determinism:</li>
<li>Sharing</li>
<li>Access violation</li>
<li>Information flow</li>
</ul>
<p>Paper provides analysis of common attacks from these features. Analyzes attack models based on a finite state machine that transitions between states through: setup, interaction, transmission, reception, and decoding.</p>
<p><strong><a href="https://mici.hu/papers/bognar23microprofiler.pdf">MicroProfiler: Principled Side-Channel Mitigation through Microarchitectural Profiling</a><a href="https://www.ieee-security.org/TC/EuroSP2023/program.html#eurosp2017-35"></a></strong><br>
<strong>Marton Bognar</strong> (KU Leuven), <strong>Hans Winderix</strong> (KU Leuven), <strong>Jo Van Bulck</strong> (KU Leuven), <strong>Frank Piessens</strong> (KU Leuven)</p>
<p>Interrupting timing attacks can be done by adding dummy nop commands after a test case fails, so that the attacker can&rsquo;t tell which path was followed down. However on some platforms allow for timing of all the individual instructions, which would be different for the nop condition.</p>
<p><strong><a href="https://arxiv.org/pdf/2203.04277.pdf">You Cannot Always Win the Race: Analyzing mitigations for branch target prediction attacks</a><a href="https://www.ieee-security.org/TC/EuroSP2023/program.html#eurosp2017-36"></a></strong><br>
<strong>Alyssa Milburn</strong> (Intel Corporation), <strong>Ke Sun</strong> (Intel Corporation), <strong>Henrique Kawakami</strong> (Intel Corporation)
Modern processors with a high number orat</p>
<p><strong>From Dragondoom to Dragonstar: Side-channel Attacks and Formally Verified Implementation of WPA3 Dragonfly Handshake<a href="https://www.ieee-security.org/TC/EuroSP2023/program.html#eurosp2017-38"></a></strong><br>
<strong>Daniel De Almeida Braga</strong> (Université de Rennes 1, CNRS, IRISA), <strong>Mohamed Sabt</strong> (Université de Rennes 1, CNRS, IRISA), <strong>Pierre-Alain Fouque</strong> (Université de Rennes 1, CNRS, IRISA), <strong>Natalia Kulatova</strong> (Mozilla), <strong>Karthikeyan Bhargavan</strong> (INRIA)</p>
<p>Issues with password conversion method in WPA3 resulting in a new offline dictionary attack.</p>
<h2 id="session-crypto--formal-methods-ii">Session: Crypto + formal methods II</h2>
<p><strong><a href="https://arxiv.org/pdf/2208.00283.pdf">Recurring Contingent Service Payment</a><a href="https://www.ieee-security.org/TC/EuroSP2023/program.html#eurosp2017-39"></a></strong><br>
<strong>Aydin Abadi</strong> (University College London), <strong>Steven J. Murdoch</strong> (University College London), <strong>Thomas Zacharias</strong> (University of Edinburgh)</p>
<p>Fair exchange where two mutually distrustful parties want to swap digital items such that neither party can cheat the other. Blockchain-based solutions for fair exchange, by paying in digital currencies. Current solutions lack privacy and reveal real-time information about parties.</p>
<p><strong><a href="https://eprint.iacr.org/2022/1023.pdf">SIM: Secure Interval Membership Testing and Applications to Secure Comparison</a><a href="https://www.ieee-security.org/TC/EuroSP2023/program.html#eurosp2017-40"></a></strong><br>
<strong>Albert Yu</strong> (Purdue University), <strong>Donghang Lu</strong> (Purdue University), <strong>Aniket Kate</strong> (Purdue University), <strong>Hemanta K. Maji</strong> (Purdue University)</p>
<p>Secure comparison is used in neural network and decision tree applications which try to be privacy preserving and secure. Want to split work between offline and online, and push work to the offline phase where things can be pre-computed.</p>
<p><strong>Careful with MAc-then-SIGn: A Computational Analysis of the EDHOC Lightweight Authenticated Key Exchange Protocol<a href="https://www.ieee-security.org/TC/EuroSP2023/program.html#eurosp2017-41"></a></strong><br>
<strong>Felix Günther</strong> (ETH Zurich), <strong>Marc Ilunga Tshibumbu Mukendi</strong> (Trail of Bits)</p>
<p><strong>Proof-of-Learning is Currently More Broken Than You Think<a href="https://www.ieee-security.org/TC/EuroSP2023/program.html#eurosp2017-42"></a></strong><br>
<strong>Congyu Fang</strong> (University of Toronto and Vector Institute), <strong>Hengrui Jia</strong> (University of Toronto and Vector Institute), <strong>Anvith Thudi</strong> (University of Toronto and Vector Institute), <strong>Mohammad Yaghini</strong> (University of Toronto and Vector Institute), <strong>Christopher A. Choquette-Choo</strong> (Google), <strong>Natalie Dullerud</strong> (University of Toronto and Vector Institute), <strong>Varun Chandrasekaran</strong> (Microsoft Research &amp; University of Illinois Urbana-Champaign), <strong>Nicolas Papernot</strong> (University of Toronto and Vector Institute)</p>
<p>Infinitesimal update attack sends a small weight update as a proof of learning.</p>
<p><strong>Certifiably Vulnerable: Using Certificate Transparency Logs for Target Reconnaiss/ance<a href="https://www.ieee-security.org/TC/EuroSP2023/program.html#eurosp2017-64"></a></strong><br>
<strong>Stijn Pletinckx</strong> (University of California, Santa Barbara), <strong>Thanh-Dat Nguyen</strong> (Delft University of Technology), <strong>Tobias Fiebig</strong> (Max Planck Institute for Informatics), <strong>Christopher Kruegel</strong> (University of California, Santa Barbara), <strong>Giovanni Vigna</strong> (University of California, Santa Barbara)</p>
<p>Honeypot study. Renewing certificates and logging on public certificate logs results in increases in traffic. Comparing to study from 3 years ago shows faster scans and IPv6 which wasn&rsquo;t in previous results. Conclusion is that these public certificate logs are still beneficial but can result in attack attempts when they are used.</p>
<h3 id="session-web-and-social-media">Session: Web and social media</h3>
<p><strong>Chrowned by an Extension: Exploiting the Chrome DevTools Protocol<a href="https://www.ieee-security.org/TC/EuroSP2023/program.html#eurosp2017-43"></a></strong><br>
<strong>José Miguel Moreno</strong> (Universidad Carlos III de Madrid), <strong>Narseo Vallina-Rodriguez</strong> (IMDEA Networks/AppCensus), <strong>Juan Tapiador</strong> (Universidad Carlos III de Madrid)</p>
<p><strong><a href="https://www.danielwoods.info/assets/pdf/KVW_DarkDialogs_EuroSnP.pdf">DarkDialogs: Automated detection of 10 dark patterns on cookie dialogs</a><a href="https://www.ieee-security.org/TC/EuroSP2023/program.html#eurosp2017-44"></a></strong><br>
<strong>Daniel Kirkman</strong> (University of Edinburgh), <strong>Kami Vaniea</strong> (University of Edinburgh), <strong>Daniel W Woods</strong> (University of Edinburgh)</p>
<p>Analysis of dark patterns in online cookie consent dialogs.</p>
<p><strong><a href="https://arxiv.org/abs/2206.14855">SoK: Content Moderation in Social Media, from Guidelines to Enforcement, and Research to Practice</a><a href="https://www.ieee-security.org/TC/EuroSP2023/program.html#eurosp2017-45"></a></strong><br>
<strong>Mohit Singhal</strong> (The University of Texas at Arlington), <strong>Chen Ling</strong> (Boston University), <strong>Pujan Paudel</strong> (Boston University), <strong>Poojitha Thota</strong> (The University of Texas at Arlington), <strong>Nihal Kumarswamy</strong> (The University of Texas at Arlington), <strong>Gianluca Stringhini</strong> (Boston University), <strong>Shirin Nilizadeh</strong> (The University of Texas at Arlington)</p>
<p>Biases and lack of training in content moderation in different dialects. Lack of consensus on what constitutes unallowed content. Facebook/Twitter have different definitions and examples of misinformation and hate speech. Most hate-speech and misinformation DNN approaches are trained on English datasets. Need of diverse participants (age, culture, gender, etc.). no one size fits all content moderation, platform users need to be treated equally. More collaborative human-AI decision making for determining misinformation and hate-speech. Ideally AI could account for the biases of humans when they are analyzing hate-speech and information.</p>
<p><strong>Been here already? Detecting Synchronized Browsers in the Wild<a href="https://www.ieee-security.org/TC/EuroSP2023/program.html#eurosp2017-47"></a></strong><br>
<strong>Pantelina Ioannou</strong> (University of Cyprus), <strong>Elias Athanasopoulos</strong> (University of Cyprus)
Browser fingerprinting allows for the unique identification of browsers by collecting device features. Browser synchronization connects different devices. Proposed algorithm detects whether http requests are from the same synchronized devices, using a user-study to test the accuracy.</p>
<h3 id="session-crypto--formal-methods-iii">Session: Crypto + formal methods III</h3>
<h3 id="1045---1210">10:45 - 12:10</h3>
<p><strong><a href="https://eprint.iacr.org/2023/419">Asynchronous Remote Key Generation for Post-Quantum Cryptosystems from Lattices</a><a href="https://www.ieee-security.org/TC/EuroSP2023/program.html#eurosp2017-48"></a></strong><br>
<strong>Nick Frymann</strong> (University of Surrey), <strong>Daniel Gardham</strong> (University of Surrey), <strong>Mark Manulis</strong> (Universität der Bundeswehr München)</p>
<p><strong><a href="https://muoitran.com/publications/revelio.pdf">Revelio: A Network-Level Attack Against the Privacy in the Lightning Network</a><a href="https://www.ieee-security.org/TC/EuroSP2023/program.html#eurosp2017-49"></a></strong><br>
<strong>Theo von Arx</strong> (ETH Zurich), <strong>Muoi Tran</strong> (ETH Zurich), <strong>Laurent Vanbever</strong> (ETH Zurich)</p>
<p><strong><a href="https://eprint.iacr.org/2023/872">Conjunctive Searchable Symmetric Encryption from Hard Lattices</a><a href="https://www.ieee-security.org/TC/EuroSP2023/program.html#eurosp2017-50"></a></strong><br>
<strong>Debadrita Talapatra</strong> (IIT Kharagpur, India), <strong>Sikhar Patranabis</strong> (IBM Research, India), <strong>Debdeep Mukhopadhyay</strong> (IIT Kharagpur, India)
Can make encrypted search strings that allow you to query encrypted databases without sharing the query.</p>
<p><strong>Provable Adversarial Safety in Cyber-Physical Systems<a href="https://www.ieee-security.org/TC/EuroSP2023/program.html#eurosp2017-51"></a></strong><br>
<strong>John H. Castellanos</strong> (CISPA Helmholtz Center for Information Security), <strong>Mohamed Maghenem</strong> (CNRS France), <strong>Alvaro Cardenas</strong> (UC Santa Cruz), <strong>Ricardo G. Sanfelice</strong> (UC Santa Cruz), <strong>Jianying Zhou</strong> (Singapore University of Technology and Design)</p>
<p><strong><a href="https://ris.utwente.nl/ws/files/303815289/main.pdf">AoT - Attack on Things: A security analysis of IoT firmware updates</a><a href="https://www.ieee-security.org/TC/EuroSP2023/program.html#eurosp2017-54"></a></strong><br>
<strong>Muhammad Ibrahim</strong> (Purdue University), <strong>Andrea Continella</strong> (University of Twente), <strong>Antonio Bianchi</strong> (Purdue University)</p>
<p>IoT devices can be attacked by uploading a different firmware. Companion aps are also a source of vulnerability since they control IoT devices.</p>
<p><strong>Comprehensively Analyzing the Impact of Cyberattacks on Power Grids<a href="https://www.ieee-security.org/TC/EuroSP2023/program.html#eurosp2017-55"></a></strong><br>
<strong>Lennart Bader</strong> (Fraunhofer FKIE &amp; RWTH Aachen University), <strong>Martin Serror</strong> (Fraunhofer FKIE), <strong>Olav Lamberts</strong> (Fraunhofer FKIE &amp; RWTH Aachen University), <strong>Ömer Sen</strong> (RWTH Aachen University &amp; Fraunhofer FIT), <strong>Dennis van der Velde</strong> (Fraunhofer FIT), <strong>Immanuel Hacker</strong> (RWTH Aachen University &amp; Fraunhofer FIT), <strong>Julian Filter</strong> (RWTH Aachen University), <strong>Elmar Padilla</strong> (Fraunhofer FKIE), <strong>Martin Henze</strong> (RWTH Aachen University &amp; Fraunhofer FKIE)</p>
<p><strong>SoK: SoK: Rethinking Sensor Spoofing Attacks against Robotic Vehicles from a Systematic View Sensor Spoofing Attacks against Robotic Vehicles from a Systematic View<a href="https://www.ieee-security.org/TC/EuroSP2023/program.html#eurosp2017-56"></a></strong><br>
<strong>Yuan Xu</strong> (Nanyang Technological University), <strong>Xingshuo Han</strong> (Nanyang Technological University), <strong>Gelei Deng</strong> (Nanyang Technological University), <strong>Jiwei Li</strong> (Zhejiang University), <strong>Yang Liu</strong> (Nanyang Technological University), <strong>Tianwei Zhang</strong> (Nanyang Technological University)</p>
<h3 id="session-trusted-computing-and-defenses">Session: Trusted computing and defenses</h3>
<p><strong>faulTPM: Exposing AMD fTPMs’ Deepest Secrets<a href="https://www.ieee-security.org/TC/EuroSP2023/program.html#eurosp2017-57"></a></strong><br>
<strong>Hans Niklas Jacob</strong> (Technische Universität Berlin), <strong>Christian Werling</strong> (Technische Universität Berlin), <strong>Robert Buhren</strong> (Technische Universität Berlin), <strong>Jean-Pierre Seifert</strong> (Technische Universität Berlin)</p>
<p><strong>CHERI-TrEE: Flexible enclaves on capability machines<a href="https://www.ieee-security.org/TC/EuroSP2023/program.html#eurosp2017-58"></a></strong><br>
<strong>Thomas Van Strydonck</strong> (KU Leuven), <strong>Job Noorman</strong> (KU Leuven), <strong>Jennifer Jackson</strong> (University of Birmingham), <strong>Leonardo Alves Dias</strong> (University of Birmingham), <strong>Robin Vanderstraeten</strong> (Vrije Universiteit Brussel), <strong>David Oswald</strong> (University of Birmingham), <strong>Frank Piessens</strong> (KU Leuven), <strong>Dominique Devriese</strong> (KU Leuven)</p>
<p><strong>Watermarking Graph Neural Networks based on Backdoor Attacks<a href="https://www.ieee-security.org/TC/EuroSP2023/program.html#eurosp2017-59"></a></strong><br>
<strong>Jing Xu</strong> (Delft University of Technology), <strong>Stefanos Koffas</strong> (Delft University of Technology), <strong>Oguzhan Ersoy</strong> (Radboud University), <strong>Stjepan Picek</strong> (Radboud University)</p>
<h3 id="13th-international-workshop-on-socio-technical-aspects-in-security-morning"><strong>13th International Workshop on Socio-Technical Aspects in Security</strong> (morning)</h3>
<p>Strength Comes in Different Shapes and Sizes: Blending Positive and Negative Security for a More Inclusive and Equitable Digital Ecosystem</p>
<p>What We Do in the Shadows: How does Experiencing Cybercrime Affect Response Actions &amp; Protective Practices? Magdalene Ng, Maria Bada and Kovila P.L. Coopamootoo</p>
<p>Increase in cybercrime sins the pandemic. Investigating responses to cybercrime victimization across different types of crime. Repeat victims are commonly studied in traditional crime, questions of how it applies to cybercrime. Gender and age differences in reactions to cyber crime and instances of re-victimization. Open-coding of types of cyber crime resulted in 8 types, and 9 types of responses. Some types of cyber crime are more common in repeated victims of cyber crimes. Most victims of multiple cyber crime experienced different types of crime.</p>
<p>As Usual, I Needed Assistance of a Seeing Person: Experiences and Challenges of People with Disabilities and Authentication Methods Ahmet Erinola, Annalina Buckmann, Jennifer Friedauer, Asli Yardim and M. Angela Sasse</p>
<p>Comparison of how different disabilities impact cybersecurity authentication methods. Often rely on assistants to complete security authentication. Some authentication methods are too difficult for certain impairments, like face ID or fingerprint ID for the visually impaired. Technological assistants and human assistants differ in how they are used to achieve authentication goals. None are fully accessible, and the disabled must use workarounds that negatively impact security.</p>
<p>Talking Abortion (Mis)information with ChatGPT on TikTok Filipo Sharevski, Jennifer Vander Loop, Peter Jachim, Amy Devine and Emma Pieroni</p>
<p>30% of tiktok users thought that at home abortions through herbs were safe. Misinformation not moderated on tiktok. How does chatGPT and knowledge of it impact how misinformation is assessed. Compared a video showing chat GTP generating misinformation about abortion herbs, vs. just the language over a picture of a flower. Another study was run after tiktok added a label that the content may be misinformation. ChatGPT was more trustworthy. Q: did anyone assess misinformation differently in the explicit/implicit conditions.</p>
<h2 id="2nd-workshop-on-active-defense-and-deception">2nd Workshop on Active Defense and Deception</h2>
<p><strong>How well does GPT phish people? An investigation involving cognitive biases and feedback</strong> Megha Sharma, Palvi Aggarwal, Kuldeep Singh, Varun Dutt</p>
<p>Phishing attacks use cognitive biases to improve efficacy of tasks. Experiment study compares human-crafted and GPT-crafted phishing emails that are additionally designed to reflect major cognitive biases. Human participants had better performance against gpt-3 crafted emails, worse against human crafted emails. This trend was observable for all of the theorized biases of human behavior. People were more confident with the Human crafted phishing emails. Q: was this effect also observable within a trial type?</p>
<ul>
<li>Related to scam-baiting using chatGPT paper: could chatGPT be used for longer conversations with people instead of just phishing. It&rsquo;s possible that chatGPT can be used since it has a low cost and is constantly getting better. However LLMs can also be used as a tool to check to see if emails are phishing.</li>
</ul>
<p><strong>Honey Infiltrator: Injecting Honeytoken Using Netfilter</strong> Daniel Reti, Tillmann Angeli, Hans Dieter Schotten</p>
<p>Tap device that injects honeytoken into application traffic. Possibility of adaptable honeypots that try to adjust behavior based on what attackers are searching for. i.e If attackers keep looking for files that don&rsquo;t exist, we could dynamically generate honeypots that fit what they are searching for.</p>
<p><strong>Towards In-situ Psychological Profiling of Attackers Using Dynamically Generated Deception Environments – A Proof-of-Concept Proposal</strong>  Jacob Quibell</p>
<p>Infer a profile of an attacker automatically based on the types of files they are trying to access. Generate documents for honeypots based on the different attacker profiles, what their motivations are, and what types of documents they would be looking for. Dynamically start up generation so that no resources are wasted. Potential for psychological inference based on individual characteristics, biases, etc.</p>
<p><strong><a href="https://keep.lib.asu.edu/_flysystem/fedora/c7/Johnson_asu_0010E_22252.pdf">Decision-Making Biases in Cybersecurity: Measuring the Impact of the Sunk Cost Fallacy to Delay Attacker Behavior</a></strong>  Chelsea Johnson</p>
<p>Sunk-cost fallacy makes attackers susceptible to wasting resources. Q: how does the risk associated with cyber deception make the sunk cost fallacy different. Participants chose a portal and solved some cypher text, and had the option to change to a new one. (wouldn&rsquo;t staying be beneficial since relearning the different cypher would take longer). Differences in the source of information about switching to the other portal.</p>
<p><strong>Learning to Defend by Attacking (and Vice-Versa): Transfer Learning in Cyber-Security Games</strong>  Tyler Malloy, Cleotilde Gonzalez</p>
<p>My presentation.</p>
<p><strong>Oral Presentation: <a href="https://arxiv.org/pdf/2108.13980.pdf">Incorporating Adaptive Deception into CyberBattleSim for Autonomous Defense</a> Using a GA-Inspired Approach</strong>  Ryan Gabrys</p>
<p>Added honeypots to cyberbattlesim environment. Evaluated different defense strategies. Generate new nodes based on a random combination of the features of the most likely nodes to be attacked. DQL is the best performing, is it using the features of the node to calculate value? could you make the new node based on an IRL approach, designing the node to maximize the RL function. Deception is less useful on longer episode length trials. Q: long-term deception: why is random performing better than credential lookups and tabular q-learning?</p>
<p><strong>Oral Presentation: From Prey to Predator: A Use Case for Using Active Defense to Reshape the Asymmetrical Balance in Cyber Defense</strong>  Pei-Yu Huang, Yi-Ting Huang, Yeali S. Sun, Meng Chang
Mittre attack. Engage framework.</p>
<h3 id="interesting-papers-for-studies-in-human-decision-making-human-ai-teaming-cognitive-modelling-reinforcement-learning-instance-based-learning-human-factors-etc">Interesting papers for studies in human decision making, human-ai teaming, cognitive modelling, reinforcement learning, instance-based learning, Human Factors, etc.</h3>
<ul>
<li>
<p><strong><a href="https://arxiv.org/abs/2206.14855">SoK: Content Moderation in Social Media, from Guidelines to Enforcement, and Research to Practice</a><a href="https://www.ieee-security.org/TC/EuroSP2023/program.html#eurosp2017-45"></a></strong> <strong>Yuan Xu</strong> , <strong>Xingshuo Han</strong> , <strong>Gelei Deng</strong>, <strong>Jiwei Li</strong>, <strong>Yang Liu</strong> , <strong>Tianwei Zhang</strong></p>
</li>
<li>
<p><strong>The Bandit’s States: Modeling State Selection for Stateful Network Fuzzing as Multi-armed Bandit Problem</strong> Anne Borcherding, Mark Leon Giraud, Ian Fitzgerald and Jürgen Beyerer</p>
</li>
<li>
<p><strong>Unsafe Behavior Detection with Adaptive Contrastive Learning in Industrial Control Systems</strong> Xu Zheng, Tianchun Wang, Samin Y. Chowdhury, Ruimin Sun and Dongsheng Luo.</p>
</li>
<li>
<p><strong>Re-Envisioning Industrial Control Systems Security by Considering Human Factors as a Core Element of Defense-in-Depth</strong> Jens Pottebaum , Jost Rossel, Juraj Somorovsky  Yasemin Acar  Rene Fahr, Patricia Arias Cabarcos , Eric Bodden  and Iris Gräßler.</p>
</li>
<li>
<p><strong>Fake it till you Detect it: Continual Anomaly Detection in Multivariate Time-Series using Generative AI,</strong> Gastón García González, Pedro Casas and Alicia Fernández</p>
</li>
<li>
<p><strong>Assessing Network Operator Actions to Enhance Digital Sovereignty and Strengthen Network Resilience: A Longitudinal Analysis during the Russia-Ukraine Conflict,</strong> Muhammad Yasir Muzayan Haq, Abhishta, Raffaele Sommese, Mattijs Jonker and Lambert J.M. Nieuwenhuis</p>
</li>
<li>
<p><strong>Strength Comes in Different Shapes and Sizes: Blending Positive and Negative Security for a More Inclusive and Equitable Digital Ecosystem</strong> Lizzie Coles-Kemp</p>
</li>
<li>
<p><strong>What We Do in the Shadows: How does Experiencing Cybercrime Affect Response Actions &amp; Protective Practices?</strong> Magdalene Ng, Maria Bada and Kovila P.L. Coopamootoo</p>
</li>
<li>
<p><strong>How well does GPT phish people? An investigation involving cognitive biases and feedback</strong> Megha Sharma, Palvi Aggarwal, Kuldeep Singh, Varun Dutt</p>
</li>
<li>
<p><strong>Towards In-situ Psychological Profiling of Attackers Using Dynamically Generated Deception Environments – A Proof-of-Concept Proposal</strong>  Jacob Quibell</p>
</li>
<li>
<p><strong>Decision-Making Biases in Cybersecurity: Measuring the Impact of the Sunk Cost Fallacy to Delay Attacker Behavior</strong>  Chelsea Johnson</p>
</li>
</ul>
]]></content></item><item><title>Security and Human Behaviour 2023</title><link>tylerjamesmalloy.github.io/posts/shb-2023/</link><pubDate>Fri, 16 Jun 2023 00:00:00 +0000</pubDate><guid>tylerjamesmalloy.github.io/posts/shb-2023/</guid><description>The following are my notes taken during the SHB workshop. I have included the names of presenters but it should be noted that these are my own general thoughs and reactions while attending these presentations, and not necessarily the views and opinions of the presenters.
Day 1 Session 1: Privacy Lorrie Cranor: Designing Visable and Useful Privacy Icons: Laura Brandimarte: Parental Trust and Automated Detection of Cyber Predators: One issue with the use of tools to ensure protection against cyber predators is algorithm aversion in the general public.</description><content type="html"><![CDATA[<p>The following are my notes taken during the SHB workshop. I have included the names of presenters but it should be noted that these are my own general thoughs and reactions while attending these presentations, and not necessarily the views and opinions of the presenters.</p>
<h2 id="day-1">Day 1</h2>
<h3 id="session-1-privacy">Session 1: Privacy</h3>
<ul>
<li><strong>Lorrie Cranor</strong>: Designing Visable and Useful Privacy Icons:</li>
<li><strong>Laura Brandimarte</strong>: Parental Trust and Automated Detection of Cyber Predators: One issue with the use of tools to ensure protection against cyber predators is algorithm aversion in the general public. People often feel like they are a part of a unique group who may be unjustly harmed by the application of algorithms, even though many people agree that the general public should have trust in algorithms. This can introduce challenges in the detection of online predation as parents may not trust algorithms to handle their children&rsquo;s interactions online.</li>
<li><strong>Sonam Samat</strong></li>
<li><strong>Tesary Lin</strong></li>
<li><strong>Geoffrey Tomaino</strong></li>
<li>Discussion</li>
</ul>
<h3 id="session-2-security">Session 2: Security</h3>
<ul>
<li><strong>Nicolas Christian</strong></li>
<li><strong>Serge Egelman</strong></li>
<li><strong>Tony Vance</strong></li>
<li><strong>Elissa Redmiles</strong></li>
<li><strong>Rainer Böhme</strong></li>
<li>Discussion</li>
</ul>
<h3 id="session-3-crime-policy-and-risk">Session 3: Crime, Policy, and Risk</h3>
<ul>
<li><strong>David Livingstone Smith</strong></li>
<li><strong>Sngus Bancroft</strong></li>
<li><strong>Damon McCoy</strong></li>
<li><strong>Li Jiang</strong></li>
<li>Discussion</li>
</ul>
<h3 id="session-4-behavior">Session 4: Behavior</h3>
<ul>
<li><strong>Coty Gonzales</strong></li>
<li><strong>Rick Wash</strong></li>
<li><strong>Julie Downs</strong></li>
<li><strong>Bart Knijneburg</strong></li>
</ul>
<h2 id="day-2">Day 2</h2>
<h3 id="session-5-users">Session 5: Users</h3>
<ul>
<li><strong>Norman Sadeh</strong></li>
<li><strong>Sophie van der Zee</strong></li>
<li><strong>Kevin Roundy</strong></li>
<li><strong>Florian Schaub</strong></li>
<li><strong>Cristobal Cherye</strong></li>
</ul>
<h3 id="session-6-trust">Session 6: Trust</h3>
<ul>
<li><strong>Avi Collis</strong></li>
<li><strong>Ahana Datta</strong></li>
<li><strong>Ananya Sen</strong></li>
<li><strong>Yixin Zou</strong></li>
<li><strong>Tyler Moore</strong></li>
</ul>
<h2 id="post-conference-thoughts">Post Conference Thoughts</h2>
]]></content></item><item><title>Learning to Learn</title><link>tylerjamesmalloy.github.io/posts/learning-to-learn/</link><pubDate>Mon, 12 Jun 2023 00:00:00 +0000</pubDate><guid>tylerjamesmalloy.github.io/posts/learning-to-learn/</guid><description>One interesting development in my research interests is the unexpected focus on &amp;rsquo;learning to learn&amp;rsquo; that many of my recent papers and projects have been focused on. During my PhD, I had shyed away from the more complex topics in cognitive modeling, such as meta learning. This decision was in part motivated by my interests in understanding the most basic and fundamental aspects of human and machine learning. Another aspect of this decision was the increased difficulty and potential problems that are associated with designing more complex tasks.</description><content type="html"><![CDATA[<p>One interesting development in my research interests is the unexpected focus on &rsquo;learning to learn&rsquo; that many of my recent papers and projects have been focused on. During my PhD, I had shyed away from the more complex topics in cognitive modeling, such as meta learning. This decision was in part motivated by my interests in understanding the most basic and fundamental aspects of human and machine learning. Another aspect of this decision was the increased difficulty and potential problems that are associated with designing more complex tasks.</p>
<p>The overall motivation behind much of my research is in bridging the gap between the understanding of how the human mind functions in reality, and how we choose to design machine learning methods. The hope is that through an increased understanding of how the human mind achieve behavioral goals, we can design more human-like machine learning methods that better reflect cognative realities. However, this motivation presupposes the usefulness of machine learning methods that are similar to the human mind.</p>
<p>The simplest justification for designing human-like ML is that humans excell at a variety of tasks, such as learning to learn, and if we can build ML models that reflect human abilities in these ways, we can improve them. However, much of the research in AI is not interested in doing things in a human-like manner, rather preferring to describe some loss function and optimize model behavior relative to it.</p>
<p>While many recent methods in machine learning, as well as classical approaches, do draw heavily from motivations related to human learning, much of the recent success in areas like Large Language Models (LLMs) appear to go against what we know about human learning and behavior. One specific example is that models that are designed for conversation and text generation typically do so by iteratively predicting the next word, and maximizing the probability of that word relative to the dataset that it is trained on. One major factor of these models is that they are stateless, meaning they aren&rsquo;t basing their pedictions based on some existing model of the world or an environment they exist in. This obviously seems very disconnected from the way that humans generate language, which is intrisically tied to the environment that they are in which defines their goals, preferences, and desires, and in turn the language that they generate.</p>
<p>One of the most relevant aspects of recent ML methods to the question of the difference between human and machine learning is model overparameterization, which can be simply described as the case where the number of parameters in a ML model exceeds the number of data instances that the model is trained on. Part of the reason why LLMs do not need the same type of highly specialized language generation structure that the human mind has is that they are overparameterized. This means that hypothetically, the same type of world model based behavior that humans rely on could be an emergent phenomenon resulting from the overparameterization of LLMs and their training regiem in predicting the next word in a sentence.</p>
<p>But there is a quesiton of whether or not the human mind is overparameterized in a similar way as LLMs. Traditionally we think of human cognition as highly limited in its capacity, which can be assumed to exist due to the highly biased behavior of humans, and the apparent biological realities of the limitations of information processing in the brain. However, in his speech to NeurIPS 2022, Geoffrey Hinton noted that &lsquo;Perhaps&hellip; we are data limited rather than capacity limited&rsquo;, due to the disparity between the experience of humans (as measured in seconds) and the relatively larger number of neurons in the brain. This raised the question of whether information capacities are sufficiently impactful in determining the behavior of humans.</p>
<p>Related to this issue of whether the human brain is overparameterized is the question of the source of meta-cognitive abilities in the human brain and ML methods. LLMs have been suggested to have meta-cognitive abilities even though they were not trained explicitly to produce meta-cognition. This raises the question of whether faculties like transfer of learning in humans emerge as a natural result of our brains being trained and optimized through evolution (though thinking of evolution as a direct optimizing force or process is not correct) and experience to achieve some goal distinct from meta-cognition. This is related to the issue of how brains vs. LLMs are trained. Predicting the next word in a sentence seems counterintuitive to assumptions of human language generation, but it seems highly connected to theories of human cognition like the free energy principle, which suggests that the brain may at its base function to predict future states.</p>
<p>Overall, I don&rsquo;t think that the differences in training and structure of humans and LLMs can be swept under the rug. There may be many emergent phenomenon in both humans and LLMs that are not explicitly reflected in the ways that these models and brains are &rsquo;trained&rsquo;, but the existence of two emergent phenomenon that produce similar behavior does not entail a structural similarity in the base models themselves. To me, there is still a value in designing ML models with a human-like justification.</p>
]]></content></item><item><title>Job Security</title><link>tylerjamesmalloy.github.io/posts/job-security/</link><pubDate>Wed, 05 Apr 2023 00:00:00 +0000</pubDate><guid>tylerjamesmalloy.github.io/posts/job-security/</guid><description>I recently completed the last meeting with a small group of mentees from my alma mater UBC that was put together to give undergraduate students at different points in their career some insight into life after undergrad. I think it was a great experience for myself, as I had little previous work as a direct mentor, and hopefully the mentees felt the same way. The remainder of this blog post is some reflections from my experience as a mentor and the life lessons I have thought of and tried to discuss with them.</description><content type="html"><![CDATA[<p>I recently completed the last meeting with a small group of mentees from my alma mater UBC that was put together to give undergraduate students at different points in their career some insight into life after undergrad. I think it was a great experience for myself, as I had little previous work as a direct mentor, and hopefully the mentees felt the same way. The remainder of this blog post is some reflections from my experience as a mentor and the life lessons I have thought of and tried to discuss with them.</p>
<p>The title of this post is partially a pun referencing the fact that I have recently begun work researching human and AI learning with a focus on applications in cybersecurity domains. If you had told me during my PhD that I would be working on that topic, I would have assumed that I left academia and began work in industry, finding the first relevant job that I could get. Interestingly, my impression of the differences between industry and academia in terms of job security and how much control one has over the type of work they do has evolved in the past year since defending my PhD (and I still haven&rsquo;t even received my robe at commencement yet).</p>
<p>One of the often discussed (potential) benefits of staying in academia after completing a PhD is that one has more control over the type of work they do. I think that oftentimes this is assumed by PhD students, as well as people who never entered academia in the first place, to be a general statement about control at all levels, both the high motivation level and the lower applied level. However, the control that one has over the type of work you do is often different across these levels, and there is no one-size fits all description of how researchers in academia have control over their work compared to researchers in industry or government.</p>
<p>Since I have begun work in security, I am fascinated and amazed at the potential for using it as a domain for better understanding the similarities and differences between human and AI learning. For me, I was able to keep one aspect of the high level motivation of my research interests, while shifting the application to fit a new (and amazing) team of research collaborators. Again, for me, this was a somewhat easy change, since I had not been dead-set on a specific application of my research interests during my PhD. Part of my work initially was in robotics applications, and then visual learning and decision making. This meant that I was a good fit for the position that I applied for, and will (hopefully) be a good addition to the future research of this group.</p>
<p>The other meaning of the (punny) blog post is in reference to the recent downturn in hiring at large computer science companies. While I am far from an expert in determining the job market for different sectors, this was partially a reminder that nothing is guaranteed, and choosing one path over another because of a perceived benefit might not always be the best. For me, I could have easily left academia in pursuit of an industry job only to find myself out of work soon after. But since I chose to stay, there is no way to ever know what would have happened.</p>
<p>However, my story is just that, my own, and doesn&rsquo;t mean that other people who do postdocs or researcher positions in academia have or want to have the same level of control over their research. Many researchers want to totally shift both the domain and application of their research, even significantly changing the area of science they study. Others only care about the low level aspects and want to change some way that research is applied to some specific real world application. These are all possible results of pursuing a PhD, and oftentimes the preferences that one has for the type of research control they have in the future is a fluidly changing thing.</p>
<p>This is all to say that I am very happy with my current position working in cyber security and other applications of human and AI learning. While I could have never guessed I would be working on this topic, at every step of my journey I have tried to do what seemed to me like the best options in relation to my preferences for both my life and research. I think that oftentimes people hold strongly to what they perceive as their preferences for their work or the future, and this can limit them in what they are willing to explore. This is especially true in research, which for the sake of good science shouldn&rsquo;t be the case.</p>
<p>Also since the remaining aspect of the (punny) title is a reference to the doom and gloom over chat gpt replacing everyone’s jobs, I have again asked chat gpt to review my blog post and it had the following to say:</p>
<p>Chat GPT: Overall, this blog post provides interesting reflections on the author&rsquo;s experience as a mentor and their personal journey transitioning from academia to industry, with a focus on their current work in cybersecurity and AI learning. The post also touches on the importance of being open-minded and adaptable when it comes to career choices and preferences.
The author effectively uses a pun in the title to introduce the topic of their work in cybersecurity and AI learning, and explains how their research interests have evolved over time. They provide a nuanced perspective on the differences between control over one&rsquo;s work in academia versus industry, highlighting that it is not a one-size-fits-all description.
The author&rsquo;s personal story adds a relatable and authentic touch to the post, and their reflection on the potential downsides of holding strongly to perceived preferences for work and the future is a valuable lesson.
One potential area for improvement is that the post could benefit from more concrete examples or actionable advice for the mentees or other readers. The post is largely focused on the author&rsquo;s personal experience and reflections, but it would be helpful to have some practical tips or recommendations for those looking to navigate their own career paths.
Overall, this blog post provides an engaging and thought-provoking perspective on career choices and the importance of staying open-minded and adaptable.</p>
<p>Tyler: Interestingly, it seems to be under the impression that I have left academia for industry! I was trying to give just the opposite impression in my blog post. Also, the recommendations for improvements seem somewhat unrelated to what I was trying to describe with the post, as the general theme is that direct recommendations may not be useful due to changing preferences and life plans. Maybe jobs are a bit more secure than it would seem from the news.</p>
]]></content></item><item><title>Narrowing The Gap With LLMs</title><link>tylerjamesmalloy.github.io/posts/narrowing-gap/</link><pubDate>Tue, 14 Feb 2023 00:00:00 +0000</pubDate><guid>tylerjamesmalloy.github.io/posts/narrowing-gap/</guid><description>This will be my third blog post in a row (see the first, and second) on the topic of large language models. While this area of research has been in the news significantly recently, it is not exactly my area. However, there was a talk at the University of Pittsburgh philosophy department given by professor Colin Allen that was, at least partially, presented as a refutation to the position of David Chalmers that I discussed previously.</description><content type="html"><![CDATA[<p>This will be my third blog post in a row (see the <a href="https://tylerjamesmalloy.github.io/posts/neurips-2022/">first</a>, and <a href="https://tylerjamesmalloy.github.io/posts/sentience/">second</a>) on the topic of large language models. While this area of research has been in the news significantly recently, it is not exactly my area. However, there was a talk at the University of Pittsburgh philosophy department given by professor Colin Allen that was, at least partially, presented as a refutation to the position of David Chalmers that I discussed previously. Since I talked about Chalmers&rsquo; ideas in my previous blog post, I thought this would serve as a nice contrast to those ideas, and hopefully wrap up my thinking about it for the time being.</p>
<p>In Dr. Allen&rsquo;s talk, he discusses some issues with large language models and the continuing gap between AI and human intelligence. One of the arguments that Dr. Allen raises about the current issues with LLMs are the level to which they display &rsquo;thruthiness&rsquo;, a term coined by the late night comedian Stephen Colbert to refer to plausible sounding nonsense. This phenomenon can be seen on many examples of twitter posts and blog articles about the failures of chatGPT and related LLMs. In the talk, Dr. Allen presented his own example of an example code to calculate the modulus of a hexadecimal number, which chatGPT gave an incorrect response to. When Dr. Allen repeatedly told chatGPT that its response was factually incorrect, which chatGPT agreed with, however it could not provide the correct response.</p>
<p>This over-agreeable behaviour is something I have noticed when using these LLM systems, that they rarely display their own preferences and refuse to take stances on many issues or opinions. Also, they often change their mind when presented with different points of view such as an argument I recently got into with chatGPT about the differences and similarities of the film &ldquo;Blade Runner&rdquo; and the book &ldquo;Do Androids Dream of Electric Sheep&rdquo;. During that discussion, chatGPT repeatedly mentioned that the movie is more &lsquo;visual&rsquo;. When I said that that distinction isn&rsquo;t relevant when comparing a book and movie, chatGPT would initially agree with me before mentioning the same point again. Another fun problem for chatGPT is logic, which you can see an example of in a <a href="https://twitter.com/TylerJMalloy/status/1624277084544671744/photo/1">screenshot on my twitter</a>.</p>
<p>An interesting point that Dr. Allen raised is the fact that LLMs function by averaging over large courpora of natual language, which is succeptible to the &lsquo;dangers of averaging&rsquo;. Here, when we try to predict how humans are learning in a task, we may take the average performance over some number of trials of a population of human participants, and try to build a model that reflects that average. However, if we were to then conclude that the humans and the cognitive model are behaving similarly, it would ignore the fact that there is typically a wide variation in performance of any task in a given population. Cognitive models should ideally reflect these realities, and try to explain what precisely is going on in each participant that lead them to their own level of performance. While this may not be a perfectly achievable goal, or necessary for all applications of cognitive models, it is important to remember that any system build on a large among of human generated data, like LLMs, can run into these types of issues.</p>
<p>Dr. Allen at one point summarized the position of David Chalmer&rsquo;s argument that LLMs, thought not currently sentient, could be added to in specific ways that would increase the likelihood that they would display some level of sentience. Dr. Allen disagreed with this general notion by arguing that what is truly missing from LLMs, that they can&rsquo;t solve with more layers or training, is an architecture that allows for meta-cognition. It is important, to Dr. Allen, that conscious agents be able to &rsquo;take a step back&rsquo; and see why they are incorrect or correct using reasoning and logic. Currently logic problems and mathematics are big issues for vanilla LLMs, though these issues can be somewhat overcome through prompt engineering and limited training methods, as descibed in the paper I discussed in my earlier blog post on emergent world representations in LLMs.</p>
<p>Personally, I agree in the importance of a meta-cognitive architecture that allows models to use different faculties to address a problem, and oversee the reasoning used to decide what faculty to use. However, the behaviour of LLMs have already demonstrated some phenomenon that philosophy of AI researchers previously have thought would require a specific architecture and training method, rather than the relatively simple end to end structure and training of LLMs. In Chalmer&rsquo;s talk, he noted the possibility that the easiest way to train a model to do next in sentence word prediction may be to have the model learn a world representation that emerges through experience, an idea that was supported through recent studies of world representations in LLMs, mentioned in my last post. This could mean that the type of meta-cognitive architecture that Dr. Allen argues for could theoretically emerge naturally though specific training methods and larger models. However, the end result may be further and further from how meta-cognition and logic is implemented in the human brain.</p>
<p>Ultimately, I am unsure whether or not is is possible for meta-cognition, logic, compositionality, or even consciousness to emerge from the type of training and structure that current LLMs are designed on. I think that this will be an important question in the next twenty years of AI research, and it is important not to dismiss out of hand arguments in either direction. This is an exciting area of potential research that will need to bring together philosophers of the mind and AI with engineers and companies that are interested in making their products as human friendly as possible.</p>
<p>Another interesting aspect of Dr. Allen&rsquo;s presentation was that he allowed chatGPT to &lsquo;get the last word&rsquo;, by having it respond to his main arguments. I will follow in his footsteps by asking chatGPT to &lsquo;provide an objective assessment of this blog post&rsquo;, it&rsquo;s response is copied below:</p>
<p>ChatGPT:</p>
<p>The blog post provides a summary and commentary on a talk given by Professor Colin Allen on the topic of large language models (LLMs) and their limitations, in contrast to the position of David Chalmers. The post discusses issues with LLMs, such as their tendency to display &ldquo;truthiness&rdquo; and their over-agreeable behavior. The dangers of averaging and the need for a meta-cognitive architecture that allows models to use different faculties to address a problem and oversee the reasoning used are also mentioned.</p>
<p>The author of the blog post provides their personal perspective on the topic and acknowledges that the emergence of meta-cognition, logic, compositionality, or even consciousness from the type of training and structure that current LLMs are designed on is still an open question. They emphasize the importance of not dismissing arguments in either direction and the need for interdisciplinary collaboration between philosophers of the mind, AI researchers, and engineers.</p>
<p>Overall, the blog post provides an informative and balanced assessment of the talk given by Professor Allen and raises important questions and considerations regarding the current limitations and future potential of LLMs.</p>
<p>Tyler:</p>
<p>As a final note, I entered the above prompt response into the first google search result hits for GPT plagarism detectors, and both gave a 0% chance that this text was written by an AI like chatGPT. Seems like there is a ways to go before we have a clear understanding of what the possibilities of these technologies are.</p>
]]></content></item><item><title>Sentience and World Representations in LLMs</title><link>tylerjamesmalloy.github.io/posts/sentience/</link><pubDate>Tue, 24 Jan 2023 00:00:00 +0000</pubDate><guid>tylerjamesmalloy.github.io/posts/sentience/</guid><description>In my last blog post I discussed a few of the presentations given at NeurIPS 2022 that I found particularly interesting. I didn&amp;rsquo;t get a chance to write much on another presentation given by David Chalmers that was a condensed version of his earlier talk &amp;ldquo;Are Large Language Models Sentient?&amp;rdquo;. In this talk Chalmers discussed several possible positions on the question of sentience in large language models, systematically looking how these positions would define sentience and whether or not it is possible for LLMs like ChatGPT to exhibit those properties.</description><content type="html"><![CDATA[<p>In my last <a href="https://tylerjamesmalloy.github.io/posts/neurips-2022/">blog post</a> I discussed a few of the presentations given at NeurIPS 2022 that I found particularly interesting. I didn&rsquo;t get a chance to write much on another presentation given by David Chalmers that was a condensed version of his <a href="https://www.youtube.com/watch?v=-BcuCmf00_Y">earlier talk</a> &ldquo;Are Large Language Models Sentient?&rdquo;. In this talk Chalmers discussed several possible positions on the question of sentience in large language models, systematically looking how these positions would define sentience and whether or not it is possible for LLMs like ChatGPT to exhibit those properties.</p>
<p>One of the commonly quoted properties that you might require for sentience is a world model, some internal representation of the state of the world that an experiencing agent exists in. An interesting idea that was discussed by Chalmers in this talk is that, while LLMs are trained to do next word in sentence prediction, it could theoretically be the case that the best way to learn how to do that is to have a world representation that might emerge out of training these large language models.</p>
<p>A <a href="https://openreview.net/forum?id=DeG07_TcZvT">recent paper on openreview</a> for ICLR 2023 titled &ldquo;Emergent World Representations: Exploring a Sequence Model Trained on a Synthetic Task&rdquo; explores this idea of emergent phenomenon on large language models. This paper explores the question of whether or not LLMs  &ldquo;just memorize a collection of surface statistics, or do they rely on internal representations of the process that generates the sequences they see?&rdquo;. The conclusions from these experiments, using the board game othello as an environment, are that specifically trained LLMs might exhibhit an emergent property similar to a world representation that is often said to be lacking from LLMs.</p>
<p>This Chalmers talk also discusses possible additions onto LLMs (which he termed LLM+ models) that include other domains like images, videos, reinforcement learning and even theoretically other types of perception. The goal of the talk was to suggest that, even for some of the more strict definitions of what is required for sentience, we could imagine how such a LLM+ model would be constructed and trained.</p>
<p>So where does my research experience connect with these ideas? Typically we think of these world representations as being severely constrained in humans, and this could be a big difference between emergent world models in humans and  LLM+, such as the <a href="https://www.theverge.com/23560328/openai-gpt-4-rumor-release-date-sam-altman-interview">much rumored about GPT4 model</a>. A big part of my dissertation was trying to understand how the brain makes optimal representations of visual information in light of both information constraints and behavioral goals. However, if models are trained in an unconstrained manner to optimize next word prediction using billions or trillions of parameters, then any emergent phenomenon, either world models or other types of phenomenon like consciousness, might have significantly different properties than biological phenomenon.</p>
<p>While that position may sound like it is assuming that consciousness is an emergent phenomenon in humans, I don&rsquo;t want to come across as saying that is definitely the case. While it might or might not be true, I would agree on the implications of the nature of sentience or world models as they might be emerging in LLMs and other types of artificial intelligence. Specifically, if the training and optimization of these models does not taking into account information constraints, then it would likely be different from human cognition.</p>
<p>One of the possible differences exists in the example given in the &lsquo;Emergent World Representations&rsquo; paper, which used Othello games as examples for testing whether a model has a representation of the state of an Othello game. Even with this simple board game example, we can start to think of how an unconstrained LLM might represent the state of an othello game differently than the human mind. One area where the model is less constrained is in the informational content of the representation, which we can assume is in the form of textual information. Comparing this to the human mind, we might expect that some of the representation of the world state of the game is in visual information, but also that this representation could be turned into a linguistic description of the pieces on the board.</p>
<p>This difference in representations might be solved by a LLM+ type model that also includes multimodal perception like images or videos and sound. However, it might be that even that type of model would still represent the world state significantly differently due to the lack of the same types of constraints that exist in the human mind. It will be interesting to see and compare the behaviour of the next generation of LLMs and other models with what we expect and know of the human mind and consciousness.</p>
]]></content></item><item><title>NeurIPS 2022</title><link>tylerjamesmalloy.github.io/posts/neurips-2022/</link><pubDate>Mon, 05 Dec 2022 00:00:00 +0000</pubDate><guid>tylerjamesmalloy.github.io/posts/neurips-2022/</guid><description>This year I attended the Conference on Neural Information Processing Systems for the first time to present a poster alongside a paper that was accepted at the workshop on Information Theoretic Principles in Cognitive Systems.
The first day fo the conference I attended the NewInML workshop, which was intended for researchers that are new to the machine learning community. Since this was my first NeurIPS and my background is in cognitive science, this felt like the perfect way to start the conference.</description><content type="html"><![CDATA[<p>This year I attended the Conference on Neural Information Processing Systems for the first time to present a poster alongside a paper that was accepted at the workshop on Information Theoretic Principles in Cognitive Systems.</p>
<p>The first day fo the conference I attended the NewInML workshop, which was intended for researchers that are new to the machine learning community. Since this was my first NeurIPS and my background is in cognitive science, this felt like the perfect way to start the conference. The topics of this workshope varried from general overviews of researchers experiences in machine learning, to more specific topics like navigating the early years of a tenure-track position or how to best negotiate a machine learning engineer job in industry.</p>
<p>I was glad there was a wide variety of topics as it kept the long day of talks interesting, but my favorite presentation was the first of the day given by Yoshua Bengio. This consisted of a broad overview of the history of his research, including the early days of research into neural networks before the so-called AI winter. It was interesting to hear Yoshua talk about perspectives on these famous epochs in AI research and what kept him going and interested in different areas that eventually lead to some of the impressive systems we have today. While it is hard to predict the future, if I am to stay in machine learning research for a while then it could probably be expected that the field will experience something along the lines of the AI winters of the past, and staying interested in my work will be an important skill.</p>
<p>There were several interesting invited talks given during the main conference, one of my favorites was given by Geoffrey Hinton titled &ldquo;The Forward-Forward Algorithm for Training Deep Neural Networks&rdquo;. This talk was mainly focused on discussing a more biologically plausable method of training neural networks as an alternative to the backpropegation method currently in use. Hinton highlighted several issues with the biological plausibility of backpropegation, and introduced what he believes is a more biologically plausible network training method, called the feed-forward algorithm. There is an acompanying paper on Hinton&rsquo;s website that explains in detail the motivation, methods, and some preliminary experimentation on simple domains.</p>
<p>While the algorithm introduced in the talk is titled &lsquo;feed-forward&rsquo;, Hinton does not suggest that the brain is entirely feed-forward, as it is highly recurrent, but rather that it can be trained in that directional dependency, as opposed to backpropogation which lacks a biological basis. The main thrust of the model seemed to me to be similarly motivated as the free-energy principle, by relying on a model that determines the probability that the current stimuli is in a sense accurate, through the introduction of an idea of a &lsquo;goodness&rsquo; measure. This can be connected to other topics like the purpose of sleep, which Hinton suggests is both a way to train the brain to be able to generate its own experience, useful for real planning, and also as a discriminator for better labelling unreal experience.</p>
<p>One interesting point from this talk is highlighting the scale of the human cortex which he lists as roughly 10^14 connections, with &lsquo;only&rsquo; 10^9 seconds to train them, and that &ldquo;It is possible that the learning algorithm we have is not so good (as ANNs) at squeezing a lot of knowledge into a few weights, it has the opposite problem&hellip; we are data limited rather than capacity limited&rdquo;. I thought that this was an interesting point to bring up in a talk on a more biologically plausible neural networks, since most of my work on making machine learning techniques more human-like has been focused on explicitly modelling the limitations of human learning. In fact, Hinton uses the term &ldquo;Capacity-Limited&rdquo;, which has directly inspired my work in making reinforcement learning models that are closer to human like learning. However, I don&rsquo;t see the main point of this talk as totally disconnected from my own work, since it broadly is interested in more human like machine learning, while it takes a different perspective on better understanding now NNs can be trained, and my work focuses more on how humans leverage experience and biases to learn quickly, but in ways that can lead to suboptimal behaviour.</p>
<p>The workshop on Information Theoretic Princples in Cognitive Systems was created by, among others, Sam Gershman and Irina Higgins, both of whom I have cited extensively and take great inspiration from. My paper for this workshop was titled <a href="https://www.researchgate.net/publication/365203245_Learning_in_Factored_Domains_with_Information-Constrained_Visual_Representations">Learning in Factored Domains with Information-Constrained Visual Representations</a>. This paper is an extenstion of the model that I developed for my dissertation, and the human experiment that was run alongside that. The main differences with the approach used in this workshop paper is that the model uses latent representations of visual information as the input to a hypothesis generation and evaluation method, as opposed to a seperate neural network trained to predict utility. This allowed for extremely fast learning in a contextual bandit setting based on images of human faces, with the model selecting the correct option after only 2-3 experiences in the task despite the highly complex state information. I am exited to see where this research can extend to, potentially in designing a new experiment for human learning in the type of task described in this paper, as I think the proposed model would be a great explanation for human learning while also making predictions of how humans represent visual information during learning.</p>
]]></content></item><item><title>PhD Defense</title><link>tylerjamesmalloy.github.io/posts/phd-defense/</link><pubDate>Thu, 10 Nov 2022 00:00:00 +0000</pubDate><guid>tylerjamesmalloy.github.io/posts/phd-defense/</guid><description>It has been a while since I made a blog post, mostly becase I have been working hard on my PhD Dissertation and practicing for my defense, which you can watch a practice run through of at this link. Since the last time I made a post I have defended my dissertation and I am now in the final stages of my work at Rensselaer. I have had an amazing time working at RPI with my mentor Chris R.</description><content type="html"><![CDATA[<p>It has been a while since I made a blog post, mostly becase I have been working hard on my PhD Dissertation and practicing for my defense, <a href="https://youtu.be/Y9itvE0H7-Y">which you can watch a practice run through of at this link</a>. Since the last time I made a post I have defended my dissertation and I am now in the final stages of my work at Rensselaer. I have had an amazing time working at RPI with my mentor Chris R. Sims, as well as working in collaboration with the Reinforcement Learning research group at IBM, particularly with my mentor Tim Klinger. The next big step for me is starting my postdoctoral fellowship research position working with Coty Gonzalez at Carnegie Mellon in December of this year.</p>
<p>So onto the dissertation defense itself. I had practiced the presentation around a dozen or so times in the leadup to the day of the defense, and I felt that I had I good handle on the flow of the presentation. One thing that was difficult, that I assume many PhD defense presenters find, is that discussing a 100 page dissertation in 45 minutes can be very challangeing. I was lucky partially in that I had given an approximately 30 minute presentation to the Cognitive Science department at RPI only a few weeks prior, as a part of a yearly requrement for graduate students in the department.</p>
<p>That earlier presentation ended up being a preview of what I would discuss in my dissertation defense, and was very helpful in finding out what I needed to describe to someone who was unfamiliar with my disseration topic. Typically for a broad audience, like a university department, it makes sense to eschew any highly technical aspects such as loss functions used for training, or algorithms that define utility in decision making. This was my approach to the earlier introductory presentation, and I felt that a high level description of the real-world psychological phenomenon I was interested in was useful for that.</p>
<p>When it came time to begin putting together my dissertation defense, I started with this preliminary presentation, and added on any necessary detail that I would think someone in my dissertation committee, or another researcher familiar with this area, would be wondering. For me specifically, this was information on how my model was trained to predict human behaviour, and how the comparison models made their own predictions of human behaviour. This allowed me to compare in more detail my proposed model and related methods for explaining behaviour, to better explain where the differences were coming from.</p>
<p>The most significant aspect of my model that I was able to explain in more detail in the longer version of my presentation was that it is able to reflect biases present in human decision making as a result of information constraints, and not through the intoduction of additional parameters that alternative models rely on. I think that this is the easiest to understand benefit of my proposed model, and I was happy that I got to explain it in my dissertation defense. Though in order to quantify that difference, I needed to explain the 2 alternative versions of predicting human behaviour, one that does so without explicitly modelling the observed bias and one that does, and this was required for the two types of task in my experiment, as well as 2 alternative deep neural network based methods for a total of 6 comparison models.</p>
<p>Having this much more to discuss and only a slightly longer presentation time meant cutting or summarizing a lot of the content from my earlier presentation, but deciding what to cut and what to keep was easier than expected. There were a few long descriptions of real-world examples in my first presentaiton that I significantly summarized. Additionally, explaining the background of the model I proposed was shortened, given that the main audience would have (hopefully) read my dissertation. None of these things should totally be removed from an ideal dissertation defense, though explaining everything for a general audience is also not totally necessary.</p>
<p>With those changes to the presentation, and the previously mentioned dozen or so practice runs, I was able to succesfully defend my dissertation with a few minor revisons requested by my committee. During the presentation itself there were some clarifying questions from my committee that weren&rsquo;t too difficult to adreess. Afterwards the only question form the general audience came from my friend who works in vision science, wondering about the broad implications of my work for her field. I would have liked to make some strong claims about visual representations, but was more focused on learning and didn&rsquo;t want to make any claims not fully backed up by the experiment I presented.</p>
<p>After a short deliberation period I was welcomed back into the room by my advisor Chris, who had a smile on his face as he said the words I had been waiting nearly 5 years to hear &ldquo;Congratulations Dr. Malloy&rdquo;.</p>
]]></content></item><item><title>What's in a Representation?</title><link>tylerjamesmalloy.github.io/posts/whats-in-a-representation/</link><pubDate>Mon, 29 Aug 2022 00:00:00 +0000</pubDate><guid>tylerjamesmalloy.github.io/posts/whats-in-a-representation/</guid><description>As I begin the final stages of my PhD thesis, I was recently surprised by one topic that jumped out as being more relevant than I had originally thought of when I began work on the project. Broadly, the topic is in the realm of cognitive modelling, specifcially modelling and predicting the behaviour of humans performing a learning and decision making task based on visual information. Traditional approaches to predicting how humans learn and make desicions have done so by abstracting away much of the compelxity of the task presented to humans, and modelling their behaviour as some simple function of the features of the task, such as a soft-max distribution over the utilities associated with the options presented.</description><content type="html"><![CDATA[<p>As I begin the final stages of my PhD thesis, I was recently surprised by one topic that jumped out as being more relevant than I had originally thought of when I began work on the project. Broadly, the topic is in the realm of cognitive modelling, specifcially modelling and predicting the behaviour of humans performing a learning and decision making task based on visual information. Traditional approaches to predicting how humans learn and make desicions have done so by abstracting away much of the compelxity of the task presented to humans, and modelling their behaviour as some simple function of the features of the task, such as a soft-max distribution over the utilities associated with the options presented.</p>
<p>For context, when I began my PhD I was interested in two main areas, cognitive modelling and deep learning, which I had hoped to connect together throughout the work that I did during my time in grad school. This was not a wholely new approach to modelling human behaviour in complex tasks. However, the choice of how best to model cognition should not be motivated by individual interests alone. To motivate the use of deep learning approaches in modelling human cognition required a conectinon to biological percpetion, specifically vision.</p>
<p>One commonly cited and long-standing connection between human visual perception and deep learning is between the activation of convolutional neural networks and the preliminary processing of visual information that takes place in biological brains. However, recent research by Higgins et al. has investigated how visual information is represented in highly specialized areas of the brain, such as the primate face area, which processes visual information related to primate faces. Results from analysis of the activation of single neurons in the primate face area revealed a so-called disentangled property, meaning that deviation along a single dimension of input stimuli like chaning the age alone or hair color alone resulted in an altered activity of individual neurons. This is closely connected to the behaviour of beta-Variational Autoencoders which are trained to form low-dimension representations of stimuli that are information-constrained and able to reconstruct the original stimuli as accurately as possible.</p>
<p>The similarity of this highly specialized brain region to the properties of B-VAE models raises an important question of the structure of information as it is being processed by the reinforcement learning faculty of the human brain. Specifically the midbrain of humans has long been analyzed and understood as the centre of RL in biological agents, with dopamine acitivty serving as an alogue to the reward prediction error used in RL to update the predicted values associated with certin stimuli. To properly function, RL must make predictions of reward associated with stimuli, or states of an environment, and update these predictions based on experience.</p>
<p>In Deep RL this is typcially done using a CNN or other deep neural network model that processes the visual information to form a state representation, and predictions of reward are made based on these representations. However, there are many issues with applying Deep RL methods that use CNNs to predicting human behaviour, and much of the research I have done so far in my PhD has been in trying to improve the generalization and robustness used in deep RL methods when predicting human behaviour. For my final thesis project, I will be applying B-VAE models onto predicting learning and decision making in humans based on visual information. The main thrust of this research is in analyzing the representations formed by Deep RL models when they are trained to predicut human behaviour. Hopefully, this analysis will reveal the source of some common biases in human learning and decision making, as well as the good qualities of human learning such as robustness and generalization.</p>
<p>To draw this research back to the title of this blog post, I am interested in investigating what information is contained within cognitive neural representations as that information flows from the visual information processing regions of the brain into the reinfocement learning regions.</p>
<p>To hear more about this project, stay tuned for an update on my PhD thesis towards the end of the year!</p>
]]></content></item><item><title>Vss 2022</title><link>tylerjamesmalloy.github.io/posts/vss-2022/</link><pubDate>Sat, 28 May 2022 00:00:00 +0000</pubDate><guid>tylerjamesmalloy.github.io/posts/vss-2022/</guid><description>Earlier this month I attented the Visual Science Society 2022 conference where I gave a talk titled &amp;ldquo;A Beta-Variational Auto-Encoder Model of Human Visual Representation Formation in Utility-Based Learning&amp;rdquo;. The entire abstract I submitted is copied at the bottom of this blog post if you would like to read it.
The VSS experience was very positive, I was somewhat worried or hesitant that I would not get a lot of the posters and talks since my experience is much more in the learning domain rather than vision.</description><content type="html"><![CDATA[<p>Earlier this month I attented the Visual Science Society 2022 conference where I gave a talk titled &ldquo;A Beta-Variational Auto-Encoder Model of Human Visual Representation Formation in Utility-Based Learning&rdquo;. The entire abstract I submitted is copied at the bottom of this blog post if you would like to read it.</p>
<p>The VSS experience was very positive, I was somewhat worried or hesitant that I would not get a lot of the posters and talks since my experience is much more in the learning domain rather than vision. While there were many talks and posters that I didn&rsquo;t fully understand, all of the participants and presenters made a good effort to explain things for a general audience and I didn&rsquo;t have too much trouble getting the general gist of projects.</p>
<p>Apart from the general vision research, there was, to me, a suprising number of Deep Neural Network techniques used in various vision related projects, as well as a fair number of tasks that involved either learning or decision making, which is the focus of my thesis. I was able to meet with a few PIs who were presenting posters related to their work that I was very interested in and discuss how my work relates to there.</p>
<p>Another good experience was the industry meeting. While I do have experience in industry research somewhat through my internship with IBM, it was interesting to see how different companies handle AI and Human learning research. There was a good combination of bigger companies (Apple and Google), as well as smaller companies and start ups that have very different structures when it comes to research.</p>
<p>The other great aspect of VSS is of course the location, St.Pete&rsquo;s beach Florida which had amazing weather. I had heard that in previous years the weather had been very hot and somewhat muggy/humid. This year there was probably only one day where I felt too hot/humid while walking around. I did get to stay in the same hotel where the conference was so I was able to pop back to my room if things got too sweaty, which I would recommend for anyone able to when attending either VSS or another conference in a hot area in the summer. So all in all I had a great experience and I&rsquo;m excited to head to my next conference Reinforcement Learning and Decision Making next month!</p>
<p>Here is the link for the <a href="https://www.youtube.com/watch?v=g0KcUqXB43U" title="VSS Presentation"><img src="https://www.youtube.com/watch?v=g0KcUqXB43U0.jpg" alt="VSS Presentation"></a> for just my talk, for you to view.</p>
<p>Abstract:
Tyler Malloy1 (<a href="mailto:mallot@rpi.edu">mallot@rpi.edu</a>), Chris R. Sims; Rensselaer Polytechnic Institute</p>
<p>The human brain is capable of forming informationally constrained representations of complex visual stimuli in order to achieve its behavioural goals, such as utility-based learning. Recently, methods borrowed from machine learning have demonstrated a close connection between the mechanisms of visual representation formation in primate brains with the latent representations formed by Beta-Variational Auto-Encoders (Beta-VAEs). While auto-encoder models capture some aspects of visual representations, they fail to explain how visual representations are adapted in a task-directed manner. We developed a model of visual representation formation in learning environments based on a modified Beta-VAE model that simultaneously learns the task-specific utility of visual information. We hypothesized that humans update their visual representations as they learn which visual features are associated with utility in learning tasks. To test this hypothesis, we applied the proposed model onto the data from a visual contextual bandit learning task [Niv et al. 2015; J. Neuroscience]. The experiment involved humans (N=22) learning the utility associated with 9 possible visual features (3 colors, shapes or textures). Critically, our model takes in as input the same visual information that is presented to participants, instead of the hand-crafted features typically used to model human learning. A comparison of predictive accuracy between our proposed model and models using hand-crafted features demonstrated a similar correlation to human learning. These results show that representations formed by our Beta-VAE based model can predict human learning from complex visual information. Additionally, our proposed model makes predictions of how visual representations adapt during human learning in a utility-based task. Further, we performed a comparison of our proposed model across a range of parameters such as information-constraint, utility-weight, and number of training steps between predictions. Results from this comparison give insight into how the human brain adjusts its visual representation formation during learning.</p>
<p>Acknowledgements: This work was supported by NSF research grant DRL-1915874 to CRS and an IBM AIRC scholarship to TJM</p>
]]></content></item><item><title>Beyond Reward</title><link>tylerjamesmalloy.github.io/posts/beyond-reward/</link><pubDate>Thu, 07 Apr 2022 00:00:00 +0000</pubDate><guid>tylerjamesmalloy.github.io/posts/beyond-reward/</guid><description>In a previous post I mentioned an interesting paper that made the claim that much of human intelligence could be viewed under the lense of reward maximization, you can see that blog post here. This point of view may not be the most common among either psychologists or computer scientists, but it would be great news for reinforcement learning researchers who are interested in making very smart systems by training them to maximize reward.</description><content type="html"><![CDATA[<p>In a previous post I mentioned an interesting paper that made the claim that much of human intelligence could be viewed under the lense of reward maximization, you can see <a href="https://tylerjamesmalloy.github.io/ty-malloy/review/2022/01/17/Reward-Review.html">that blog post here.</a> This point of view may not be the most common among either psychologists or computer scientists, but it would be great news for reinforcement learning researchers who are interested in making very smart systems by training them to maximize reward. However, I previously discussed the training aspect of reward maximization and what I believe it can achieve. Instead of discussing the ways that we can train artificial intelligence, specifically reinforcement learning agents, in this blog post I am going to talk about the assessment of RL agents.</p>
<p>The most common way to assess the performance of RL agents is their episodic or long-term reward, usually displayed as a learning curve as the training of the agent progresses and compared against alternative RL models in a given environment. This differs from other ML techniques like classification or regression problems, two of the main applications of ML, which are typically quantified in terms of error or difference between what the model predicts and the correct solution. This is in part due to the unique nature of RL among ML and AI techniques as a semi-supervised method. We could theoretically graph the error of the NN being trained in either the Q-network or policy network that instantiates our RL agent, but that may not be as informative as the reward. For many applications reward maximization is a good metric for performance, such as playing chess or go, or controlling the electrical output of a factory, etc.</p>
<p>However, if we are interested in human-like reinforcement learning, as I am, then reward maximization may not be the best or even a particularly useful metric to judge how well we are achieving our goals. TO use a specific example I will talk about the multi-agent particle environment suite, which was used in one of my previous papers that tried to apply human-like learning onto RL agents. Those environments are shown in Figure 1 and a citation to both my paper as well as the original paper that described them is listed in the reference section. These are very interesting environments from the perspective of designing human-like RL since they require some aspect of multi-agent interaction consisting of a mix of communication in cooperative, competitive, and mixed environments. These types of communicative tasks may be simple while simultaneously revealing many important and interesting aspects of human behaviour.</p>
<!-- raw HTML omitted -->
<p><!-- raw HTML omitted --><!-- raw HTML omitted --><!-- raw HTML omitted -->Figure from my paper [2] describing the Multiagent Particle Environments [2]. Adversary: 2 Good agents and 1 adversary are rewarded by closeness to a target, good agents must not reveal which object is the target by spreading to both the target and distraction. Crypto: 1 Good agent communicates target landmarks to another good agent over a public communication channel, 1 adversary attempts to decode the communicated target. Push: 1 good agent moves towards the target landmark while avoiding 1 adversary. Reference: 2 mobile good agents communicate to determine which landmark is the target. Speaker: 1 static good agent communicates to 1 mobile good agent which landmark is the target. Spread: 3 good agents spread to cover all landmarks. Tag: 1 good agent moves to distance itself from 3 adversaries using obstacles to slow their approach. World: 2 good agents move to gather food, hide within trees, and avoid 4 adversaries, 1 adversary leader can observe good agents hiding in trees and communicate their location.<!-- raw HTML omitted --><!-- raw HTML omitted --></p>
<!-- raw HTML omitted -->
<p>In this paper, we measured the performance in the same way that many papers that utilized this suite of environments had, by taking two models and comparing the performance against each other. In cooperative games all agents are the same model, and in competitive games one team of agents is the proposed model with the other being controlled by some baseline. Results from this direct reward comparison showed some modest improvements in reward, especially in the competitive tasks which amplified any differences in ability between the two agents. However, these types of modest results are often thought of by reviewers as within the bound of what you might expect by performing some hyper-parameter optimization on a given model, and thus not typically enough to claim strongly that one model is superior to another. While true, I think that these types of assessments partially miss the point of human-like RL in general, which seeks to make the performance of RL agents more human-like where possible. Though it is understandable that one would come to this conclusion given the way that we tried to compare performance in the paper.</p>
<!-- raw HTML omitted -->
<p><!-- raw HTML omitted --><!-- raw HTML omitted --><!-- raw HTML omitted -->Reward results in MPE suite comparing our information-constrained model with a baseline in the final 1K episodes of training.<!-- raw HTML omitted --><!-- raw HTML omitted --></p>
<!-- raw HTML omitted -->
<p>Instead of continuing to use averaged rewards from thousands of randomized trials, I believe it may be more useful and perhaps necessary for the evaluation of human-like behaviour in environments that require complex social interactions and communication. Take for example the adversary environment which requires the two good agents to split up between the target location and the distractor, to prevent the bad agent from determining which is the target. If we imagine a constructed beginning state with deterministic good agents that always move towards the target for a bit before splitting up, it should be easy for the bad agent to determine which is the area of interest. However, the model I proposed in the previous paper would have no way to directly determine this based on the previously revealed goals and behaviour of these good agents. Thus this type of constructed example could be perfect to assess human-like psychologizing in our agents.</p>
<p>I hope to use these ideas as I begin my work on card games that require theory-of-mind reasoning like poker, hanabi, mahjong, and bridge. All of these are great and interesting domains that allow for the analysis of complex psychological phenomena, but simply averaging over thousands or even millions of trials may result in average rewards that suggest different models are more similar than they truly are. If we were to construct specific environment states and opponent or team mate behaviours then we could hopefully get a more fruitful assessment of performance that doesn&rsquo;t miss the forest for the trees.</p>
<h2 id="references">References</h2>
<p>[1] T. Malloy, C. R. Sims, T. Klinger, M. Liu, M. Riemer and G. Tesauro, &ldquo;Capacity-Limited Decentralized Actor-Critic for Multi-Agent Games,&rdquo; 2021 IEEE Conference on Games (CoG), 2021, pp. 1-8, doi: 10.1109/CoG52621.2021.9619081.</p>
<p>[2] R. Lowe, Y. Wu, A. Tamar, J. Harb, P. Abbeel, and I. Mordatch, “Multi-agent actor-critic for mixed cooperative-competitive environments,” in Proceedings of the 31st International Conference on Neural Information Processing Systems, 2017, pp. 6382–6393.</p>
]]></content></item><item><title>Rl Web Security</title><link>tylerjamesmalloy.github.io/posts/rl-web-security/</link><pubDate>Sat, 05 Mar 2022 00:00:00 +0000</pubDate><guid>tylerjamesmalloy.github.io/posts/rl-web-security/</guid><description>Since my partner is a web security expert, I often end up having long discussions about internet security, even though my personal knowledge and research is in a very different area. That has recently gotten us to thinking about the intersection of reinforcement learning and web security. Though at first these may seem like two disparate areas, as anyone who has had experience talking at length with another person about their specific research area will know, eventually there are many commonalities that can be found between the two.</description><content type="html"><![CDATA[<p>Since my partner is a web security expert, I often end up having long discussions about internet security, even though my personal knowledge and research is in a very different area. That has recently gotten us to thinking about the intersection of reinforcement learning and web security. Though at first these may seem like two disparate areas, as anyone who has had experience talking at length with another person about their specific research area will know, eventually there are many commonalities that can be found between the two.</p>
<p>These connections were relatively easy to make since my own personal experience in reinforcement learning research is in the area of human-inspired RL. To the best of my understanding, web security revolves around the ways that humans and automated systems interact with web based services. Because of this, RL agents designed with a human-centric approach seem relatively well suited for interacting with these systems.</p>
<p>The first use case for such RL systems that immediately jumps to mind is in performing some type of testing for existing web services or ones in development. I do have some minor experience in web application software testing, though it was in performance specifically and centered moreso around timing and user satisfaction rather than data security.</p>
<p>As with all potential use-cases of RL it is a fun mental exercise to break down the state, environments, actions, and rewards to see if we can immediately make sense of what the system would look like and how it would perform its task. Additionally this is useful in determining if existing solutions are sufficient or better suited to the tasks that are in question. We can use the classic diagram from Sutton and Barto as a reference for the interaction between these parts.</p>
<!-- raw HTML omitted -->
<p><!-- raw HTML omitted --><!-- raw HTML omitted --><!-- raw HTML omitted -->Fig.1 - Reinforcement Learning Environment Diagram, from Sutton and Barto RL, hosted by lcalem on github .<!-- raw HTML omitted --><!-- raw HTML omitted --></p>
<!-- raw HTML omitted -->
<p>For the environment, we would likely want to limit our area of investigation to a particular website or web based application/system to limit the space we are interested in. However, there are many possible structures for how information is represented on the web, and we should additionally limit, at least for our brief discussion, how this information is represented. One particularly useful manner is provided by the REST API or Representational State Transfer Application Programing Interface. Even without knowing much about it, this seems like a great way to structure our RL system, State (the focus of all RL systems) is right in the name!</p>
<!-- raw HTML omitted -->
<p><!-- raw HTML omitted --><!-- raw HTML omitted --><!-- raw HTML omitted -->Fig.1 - REST API diagram, from astera. .<!-- raw HTML omitted --><!-- raw HTML omitted --></p>
<!-- raw HTML omitted -->
<p>Not only does the REST API provide a manner for representing data, more importantly it describes how the user, in this case our RL algorithm, interacts with a web based system. Just looking at the two diagrams shown previously we can easily draw direct connections between their functionalities.</p>
<p>Generally the information contained in a REST API based response from a web server is contained in a specific format such as HTML, XML, or JSON depending on the application. If we were to build a real RL system we would likely again want to narrow down (you are likely starting to see a trend) the scope by selecting one of these formats. Alternatively we could try training a more general RL system with either a logic based preprocessing step unique to each of the representation methods, or some other information structuring approach.</p>
<p>Although it may seem like an implementation level or ML engineering based question of how exactly this information is processed prior to being inputted into the RL system, much of the current research around logical based AI, neuro-symbolic AI, fuzzy logic, logic nets and so on seems highly relevant in the case of the environment we are interested in. All of those domains would probably have a useful insight into how the information should be structured and manipulated to best replicate something like general human behavior when interacting with a web based system. However, which and how those domains impact our RL system would depend on what exactly the goals are, and while it is interesting I should limit the scope (pun intended) of this blog post and move onto the reward function.</p>
<p>Now that we have a rough sketch for our environment, actions, and how both of those are going to be represented, we can begin looking at the reward and how we would train such a system. As I mentioned at the beginning, I am most experienced in building RL systems that in some way replicate how a human might interact with a similar system, given their limited attention and cognitive resources. With our current example, this might seem like a good application in inverse reinforcement learning, using human or expert behavior as a basis for training our RL algorithm. That might be a good approach if we are interested in creating the same effect as ‘smart scripts’ that could be used to stress test a system in a similar manner to real human users.</p>
<p>Another approach that is very different is more in the domain of what initially motivated this blog post, in the area of web security. Here we have a similar environment and action space, but the goal is very different. Instead of just replicating general human interaction with a web system, we want to answer the question of how secure our system is. As before one idea immediately jumps to mind, provided by the structure of web-based CTF (capture the flag) challenges which consist of a human interacting with some web system with the goal of collecting some hidden information (the flag).</p>
<p>This type of expert level behavior may require far too much generalization to be trainable from human experience, even with all of the advances of language models and other approaches that leverage massive datasets related to a specific task. Though it does seem like the type of thing we could expect web security researchers and experts to be using as a tool in the not too distant future. Since my own experience is less on the side of web security I won’t get too much into the details for fear of getting something wrong, though I likely have made some mistakes already. But nonetheless this does seem like an interesting application to me that I’ll continue to think about as my research continues.</p>
]]></content></item><item><title>Wordl Rl</title><link>tylerjamesmalloy.github.io/posts/wordl-rl/</link><pubDate>Thu, 03 Feb 2022 00:00:00 +0000</pubDate><guid>tylerjamesmalloy.github.io/posts/wordl-rl/</guid><description>This post will discuss briefly the possiblity of constucting a reinforcement learning algorithm to play the game Wordle. Language based applications of reinforcement learning are somewhat common, though perhaps not the first thing RL researchers think of as examples of applications in RL. However, Wordle is a single player game with a discrete number of actions and states, the proverbial bread and butter of RL algorithms such as one of the first successful game players TD-gammon, which palyed backgammon.</description><content type="html"><![CDATA[<p>This post will discuss briefly the possiblity of constucting a reinforcement learning algorithm to play the game Wordle. Language based applications of reinforcement learning are somewhat common, though perhaps not the first thing RL researchers think of as examples of applications in RL. However, Wordle is a single player game with a discrete number of actions and states, the proverbial bread and butter of RL algorithms such as one of the first successful game players TD-gammon, which palyed backgammon.</p>
<p>So it seems like RL may be a good idea for a AI mehtod, but how to acheve that isn&rsquo;t totally clear. I won&rsquo;t be discussing a clear implementation or writing any code in this post, just discussing the possibility. Before describing the game it should be noted that wordle is definitely possible to learn using RL, but what isn&rsquo;t clear is whether it is a good idea to do it or if there is a much better approach without RL.</p>
<p>Wordle is a simple word guessing game with the following instructions:</p>
<p>Guess the WORDLE in 6 tries.</p>
<p>Each guess must be a valid 5 letter word. Hit the enter button to submit.</p>
<p>After each guess, the color of the tiles will change to show how close your guess was to the word.</p>
<p>There are some interesting additional rules that are not included in this list of instructions. Firstly, the exact colors that the tiles change to and their meaning. A green highlighted color means that the letter you guessed is correct and in the correct location. Yellow means that the letter is correct but in the wrong location. And grey means that the letter does not appear in the target word. This is important from a reinforcement learning perspective as the state used to determine the optimal action should ideally contain this information.</p>
<p>These color codes are important for the game from a reinforcement learning perspective, but there are additional rules that limit what can be entered by the player. Firstly, words entered need to be in the valid list of words, more on that later as it is important when thinking about training a RL agent. Secondly, letters that have previously been shown to not be a member of the list (grayed out) are possible to be entered in. This is interesting as it allows for technically suboptimal behaviour, but this makes sense given the game is designed for humans who may be trying to enter one of the only words they can think of to see if any of the letters in it match and gain more inforamtion. We are only just describing how the game works in a basic sense and we are already talking about the difference between a human and AI player! Clearly this was destined as I am the one writing it, but I thought that was interesting nonetheless.</p>
<p>So if we were to design an agent we would definitelty want to be able to train it not just on the relatively short list of previous wordle answers, especially since those are unlikely to be repeated. There doesn&rsquo;t seem to be an official list of 5 letter words that are used in wordle, but there is a list of a high number of 5 letter english words, which may be the best bet as dictionaries change from year to year. <a href="https://eslforums.com/5-letter-words/">An incomplete list can be found here.</a> So with that the RL agent seems relativly straightforward right? We have our observation space and a way to get new episodes for training from that long list. But wait, what exatly is our action space?</p>
<p>If you know a bit about RL you may have an idea, but it may not be the one someone else is thinking of as there are, at least to me, two clear possibilities. Firstly the action space could be one of the roughly three thousand words in that long list I mentioned earlier. That may seem like a lot but there are domains I have worked with that had about that many actions, and other domains have far more or continuous actions which are a whole different issue. An alternative to this would be treating an action as entering a single letter. That may seem like a huge improvement in the complexity of the task, 3K to only 26! However remember that the entire word needs to be entered. So really to enter one word there are 26^5=11881376 different action combintaitons. This isn&rsquo;t as big an issue as it seems becasuse the state will be changing as the actions are made, the agent isn&rsquo;t required to pick the combination before they start typing. However from this we can see that the choice of action representation is not directly clear.</p>
<p>Since I am interested in human learning and machine learning, specifically in human inspired machine learning, it is interesting to think about how a human inspired RL method would tackle this problem. It may seem like humans definitely pick a word first and then type it out, giving credence to the worde based action space method. However, someone who has read strategies on wordle may recognize the strategy of copying down the green letters and putting X&rsquo;s in all the other spaces to allow you to visualize what the correct letter is. This method may suggest that people start typing out the letters they know, either at the beginning or end of the word, and then try to think of what the word might be.</p>
<p>There are many other topics that are interesting to think about from a human learning perpective, like trying to do imitation learning on examples from human players. There are also additionaly little quirks of the game like knowing how many of a letter there are based on the number of yellow highlighted letters in a word that had two of those letters in it. As a whole, I think this may be an example where RL might not be the best approach, as it is possible to simply have a list of all 5 letter words and remove the ones that don&rsquo;t fit the known information. This alone won&rsquo;t make a optimal player as they should additionally pick options that are maximally informative, but that could be acomplished with a bayesian approach or a planning based approach. Either way, I am confident an RL agent could play this game, but it might just not be the most interesting or useful application.</p>
]]></content></item><item><title>Is Reward Enough</title><link>tylerjamesmalloy.github.io/posts/is-reward-enough/</link><pubDate>Mon, 17 Jan 2022 00:00:00 +0000</pubDate><guid>tylerjamesmalloy.github.io/posts/is-reward-enough/</guid><description>In this post I provide a review and opinion on the paper &amp;ldquo;Reward Is Enough&amp;rdquo; by D Silver, S Singh, D Precup, and RS Sutton. In this work, the authors provide a broad perspective on reinforcement learning research and put forward the opinion that much of the behavior that interests cognitive science and artificial intelligence researchers can be viewed in relation to reward. Specifically, they propose that many cognitive faculties such as perception, language, generalization, imitation, and even general intelligence can be achieved through reward maximization and experience in an environment.</description><content type="html"><![CDATA[<p>In this post I provide a review and opinion on the paper <a href="https://www.sciencedirect.com/science/article/pii/S0004370221000862">&ldquo;Reward Is Enough&rdquo;</a> by D Silver, S Singh, D Precup, and RS Sutton. In this work, the authors provide a broad perspective on reinforcement learning research and put forward the opinion that much of the behavior that interests cognitive science and artificial intelligence researchers can be viewed in relation to reward. Specifically, they propose that many cognitive faculties such as perception, language, generalization, imitation, and even general intelligence can be achieved through reward maximization and experience in an environment. They describe a hypothesis alongside these claims that is essentially stated in the short title, that reward is enough to learn these types of complex behaviors. The following figure borrowed from the paper describes several phenomenon which could hypothetically be trained through reward based reinforcement style learning.</p>
<!-- raw HTML omitted -->
<p><!-- raw HTML omitted --><!-- raw HTML omitted --><!-- raw HTML omitted -->Fig.1 - Reward Is Enough
David Silver, Satinder Singh, Doina Precup, Richard S.Sutton (paper on ScienceDirect). This figure demonstrates the overlap in behaviour that can conceviably be taught through reward signals in a cognitive vs. artificial agent.<!-- raw HTML omitted --><!-- raw HTML omitted --></p>
<!-- raw HTML omitted -->
<p>For a cognitive psychologist or cognitive philosopher the first impression of this claim and the themes of the paper may be somewhat negative. Haven’t Chomsky and others taught us that experience in language use alone cannot give us the tools we need to be good language users? There should be a requirement that a universal grammar exist to reduce the hypothesis space of possible grammars before we even begin understanding the utterances of others, let alone generating our own. In point of fact there is a significant dearth of ‘negative’ examples of proper language, as most of our experience is with well formed language. Furthermore, much of our experience with language happens internally, without a clearly defined external reward signal.</p>
<p>Although these potential issues can be raised when taking the reward-is-enough hypothesis as a general claim or descriptive thesis on human cognition, in reality the goals of the paper are in showing that reward is sufficient for complex behavior learning. Because of this, the main purpose of the paper could be supported even without any evidence of reward based learning in a human agent. Instead the paper seeks to provide evidence that artificial reinforcement learning agents could hypothetically learn the complex behavior that humans achieve through reward signals alone.</p>
<p>While it is true that the paper claims to be more interested in describing the ‘sufficient’ aspect of learning behavior through reward, it does at the same time make some claims and connections to cognitive science that are more controversial. One source of potential controversy is the section entitled “What else, other than reward maximisation, could be enough for intelligence?” In this section the authors provide brief outlines of alternative hypotheses and suggest that they are not as well fit for training goals of general AI and other interesting behaviors. The presence of this section raises the question of the true intentions of the paper as a whole. If as the authors claim the hypothesis is centered around how behavior could be taught and not a description of human cognition, then why iterate through a list of alternatives and claim they cannot do what reward alone can?</p>
<p>In particular, the brief sentences on the free-energy hypothesis arguably leave out some important claims by cognitive psychologists such as Karl Friston in his paper “The free-energy principle: a unified brain theory?” The full quote from ‘Reward Is Enough’ is as follows: “Maximisation of free energy or minimisation of surprise may yield several abilities of natural intelligence, but does not provide a general-purpose intelligence that can be directed towards a broad diversity of different goals in different environments. Consequently it may also miss abilities that are demanded by the optimal achievement of any one of those goals (for example, aspects of social intelligence required to mate with a partner, or tactical intelligence required to checkmate an opponent).”</p>
<p>It seems from Friston’s perspective that free energy alone could provide much of what reward does for the authors of this paper. At least this should have more of a discussion if the authors are interested in showing why reward is unique in its ability. If they are not then it may be better to eschew a discussion of alternatives or claims that reward is specifically unique in its position. Otherwise, it is hard to not make larger connections of the claims made in the paper to a description of human cognition.</p>
<p>To me the general theme of the paper actually reminded me much of Karl Friston’s paper previously mentioned. There are many complex behaviors that could be defined as achieving something like free-energy minimization or reward maximization. Generally this is a good place to start for machine learning or artificial intelligence research because if you can begin to define what desirable behaviour looks like in terms that resemble something like a trainable loss function, then you are well on your way to making a useful system. This may be interesting from an AI researcher or engineering perspective, but for a cognitive psychologist the claims could be seen as somewhat vacuous and less useful for understanding cognition.</p>
<p>If you are interested in my perspective, I think that at least for much of human learning and decision making there is a combination of reward driven reinforcement style learning and predictive processing used for planning and beliefs about future states. However, I will add that I make no claims of how language and general intelligence specifically can and should be taught, as those are outside of the realm of my particular experience. I would imagine that much of those more complex behaviors is more driven by the structure of cognitive architectures as they have been optimized through millions of years of evolution.</p>
]]></content></item><item><title>Masters Thesis 2020</title><link>tylerjamesmalloy.github.io/posts/masters-thesis-2020/</link><pubDate>Tue, 28 Dec 2021 00:00:00 +0000</pubDate><guid>tylerjamesmalloy.github.io/posts/masters-thesis-2020/</guid><description>This post is another retrospective, but instead of a conference or journal paper it takes a look at my masters thesis, titled &amp;ldquo;Modelling Learning and Decision Making Under Information Processing Constraints&amp;rdquo;. This blog post will go through the begninning stages of the project and how it ultimately narrowed down the focus of the thesis and project into what it eventually became.
Very early on in my PhD, I became interested my advisor Chris Sims&amp;rsquo; previous work using infromation theory, specifically mutual information, as a tool to understand the cognitive costs of behaviour in learning and decision making tasks.</description><content type="html"><![CDATA[<p>This post is another retrospective, but instead of a conference or journal paper it takes a look at my masters thesis, titled &ldquo;Modelling Learning and Decision Making Under Information Processing Constraints&rdquo;. This blog post will go through the begninning stages of the project and how it ultimately narrowed down the focus of the thesis and project into what it eventually became.</p>
<p>Very early on in my PhD, I became interested my advisor Chris Sims&rsquo; previous work using infromation theory, specifically mutual information, as a tool to understand the cognitive costs of behaviour in learning and decision making tasks. While this may seem somewhat narrow, there are many possible applications of information theory in this way, and a wide range of psychological experimentation that has been done looking into modelling constrained cognition. We became interested in bandit learning tasks due to their simplicity, long history, and the publically available datasets with human participant responses. Since this was early on in my PhD and a while before I would eventually propose my own experiment to run, it made sense to test the ideas I had on another similar task that could guide my research in my PhD.</p>
<p>This inspired the project I worked on that would become an abstract paper in the Reinforcement Learning and Decision Making conference, and a slight extension of that paper is essentially the final section of my masters thesis. However, when I began the work in writing up my thesis, I realized I had become interested in the decision making setting during my economic modelling course. A large portion of this course was modelling decision making under risk and uncertainty, which is closely related to the decision making that takes place when learning in the bandit setting. I was wondering if the same concepts of mutual information and behavioural complexity could be used in this slightly different setting.</p>
<p>The key difference in the decision making setting is that the outcome utiltiies and probabilities that determine optimal behaviour are given directly, instead of being learned through experience as in the learning setting. This second interest and application ended up being roughly a third to a half of the content in my thesis, as it required a long background on decision making in various settings, which are some of the oldest and most well studied phenomenon in cognitive science. Taking a look back at my thesis, I realize it may seem like a lot of extra work to include only a slightly different phenomenon, but I am glad I did include it as it allowed me to relate and contrast my understanding of cognition with similar accounts from a wide variety of cognitive modells.</p>
<p>Broadly this experience in extending my ideas into related domains taught me that it is important to show as broad an application as possible, or at least as you find interesting. Often these models and approaches are applied to a very small domain which can make it difficult to relate to human cognition as we know it is extremely broad in its application. Additionally I was able to expand on my knowledge of cognitive modelling methods in similar domains as the ones I had experience in, which I am similarly greatful for.</p>
]]></content></item><item><title>AAAI 2021</title><link>tylerjamesmalloy.github.io/posts/aaai-2021/</link><pubDate>Sun, 26 Dec 2021 00:00:00 +0000</pubDate><guid>tylerjamesmalloy.github.io/posts/aaai-2021/</guid><description>This is the second post in a series of retrospectives on previous work I have done that shaped my PhD and are related to my future research goals. If you would like to read the paper you can find it on my Researchgate.
Although this paper is relatively recent, I thought I would use it as my second post because it is closely realted to a project that I began soon after beginning my AI Researcher position with IBM in early 2019.</description><content type="html"><![CDATA[<p>This is the second post in a series of retrospectives on previous work I have done that shaped my PhD and are related to my future research goals. If you would like to read the paper you can find it on my <a href="https://www.researchgate.net/publication/354551984_Capacity-Limited_Decentralized_Actor-Critic_for_Multi-Agent_Games">Researchgate</a>.</p>
<p>Although this paper is relatively recent, I thought I would use it as my second post because it is closely realted to a project that I began soon after beginning my AI Researcher position with IBM in early 2019. I actually began this project before I presented the previous project at RLDM 2019. This earlier project, which eventually became an <a href="https://www.researchgate.net/publication/349345269_Consolidation_via_Policy_Information_Regularization_in_Deep_RL_for_Multi-Agent_Games">arxiv paper</a>, was influential in my interest in robotics, machine learning, and applications of reinforcement learning.</p>
<p>After this earlier project applying reinforcement learning to a robotics simulation environment, I extended the general idea into the project and paper that goes along with it &ldquo;Capacity-Limited Decentralized Actor-Critic for Multi-Agent Games&rdquo;. This was a fortunate paper as it us to have a published work on mutual information regularized reinforcement learning. This was the main idea around the earlier metod applied to robotics. Essentially the main idea is that we want to make the behaviour learned by robots as informationally simplistic as possible. This is inspired by human behaviour which we believe strives to be similarly informationally simplistic where possible.</p>
<p>This main idea was applied into a set of complex multi-agent environments with agents using a continuous control paradigm, as well as a degree of communication in some environments. We showed in our experimentation that applying a penalty to agent behavioural complexity at the learning stage can improve learning speed as well as the eventual highest performance of the agent. Compared to agents without this complexity constraint, our agents learned more generalizable behaviour that was simultaneously less informationally complex.</p>
<p>This was a great result for our ideas of human-inspired reinforcement leanring, and I will probably make a seperate blog post in the future explaining those ideas in the future. Since this project, our ideas of human-inspired RL have slightly shifted, but this was an important first step. Recently in our work applying lessons from how humans learn complex tasks onto reinforcement learning we have focused this &lsquo;information constraint&rsquo; view in different ways, but in spirit this is the same concept as these earlier papers.</p>
]]></content></item><item><title>Predicting Human Choice</title><link>tylerjamesmalloy.github.io/posts/predicting-human-choice/</link><pubDate>Wed, 15 Dec 2021 00:00:00 +0000</pubDate><guid>tylerjamesmalloy.github.io/posts/predicting-human-choice/</guid><description>This is the first post in a series of retrospectives looking back at papers and conferences I have attended. Now that I am entering my final year of my PhD, I will be begining this as a chronicle of projects, papers and conferences that influenced my time during my PhD.
This was my first accepted paper, I submitted it to the conference Reinforcement Learning and Decision Making in 2019. I attended the conference which was a great experience.</description><content type="html"><![CDATA[<p>This is the first post in a series of retrospectives looking back at papers and conferences I have attended. Now that I am entering my final year of my PhD, I will be begining this as a chronicle of projects, papers and conferences that influenced my time during my PhD.</p>
<p>This was my first accepted paper, I submitted it to the conference Reinforcement Learning and Decision Making in 2019. I attended the conference which was a great experience. I didn&rsquo;t know then but it would be one of the few in-person conferences I would attend! Although many of the online conferences I had been to since have been great experiences, the value of in-person poster and talk presentations can&rsquo;t be understated. I hope to go back to in-person conferences before the end of my PhD.</p>
<p>Now onto the contents of this paper. Looking back at it, I didn&rsquo;t know it then but this would be a big impact on the direction of my research moving forward. This paper took a data set that had generously been made publically available by the <a href="https://nivlab.princeton.edu/">Niv Lab</a> at Princeton. Since it was my first year of my PhD, I didn&rsquo;t have the direction yet to design an experiment with human participants, and this helped significantly. The motivation of my paper was looking into modelling human learning using more modern approaches that had recently been gaining attention in reinforcement learning research.</p>
<p>The specific method I looked into was actor-critic reinforcement learning, which had been used in both artificial intelligence research as well as some cognitive modelling approaches. The difference in predictive accruacy was modest, but from my perspective the paper serves to support the body of work that uses more complex and modern reinforcement learning and artificial intelligence methods for cognitive modelling. This contribution was the focus of my paper writing and my discussions during my poster presentation.</p>
<p>Since this conference, I have continued to use this dataset in testing the model I am currently working with on my PhD thesis. Additionally, the design of the learning task inspired in part the experiment that I will soon be running. It&rsquo;s nice to look back at this early conference paper and think about how it has shaped my research moving forward.</p>
<p>Check out the <a href="https://www.researchgate.net/publication/335663034_Predicting_Human_Choice_in_a_Multi-Dimensional_N-Armed_Bandit_Task_Using_Actor-Critic_Feature_Reinforcement_Learning">paper on my researchgate profile here</a> if you are interested.</p>
]]></content></item><item><title>New Year 2022</title><link>tylerjamesmalloy.github.io/posts/new-year-2022/</link><pubDate>Sun, 03 Jan 2021 00:00:00 +0000</pubDate><guid>tylerjamesmalloy.github.io/posts/new-year-2022/</guid><description>Rather than look back at previous research I have done, as the previous posts on this blog have done, this post will look forward to my hopes for 2022 and new research ideas I am interested in. Firstly, the major plans for this year include completing the website hosting my thesis project, submitting a paper based on my work in Theory of Mind for reinforcement learning, and completing my PhD Thesis.</description><content type="html"><![CDATA[<p>Rather than look back at previous research I have done, as the previous posts on this blog have done, this post will look forward to my hopes for 2022 and new research ideas I am interested in. Firstly, the major plans for this year include completing the website hosting my thesis project, submitting a paper based on my work in Theory of Mind for reinforcement learning, and completing my PhD Thesis. Aside from that, on a more personal level I will be taking some time this year to look for possible post-doc positions or research centric positions in other areas. As a part of that I hope to continue with this blog and additionally go back to some of my previous posts and add a bit of information. Additionally I hope to expand on some of my background knowledge of other areas of cognitive science, psychology, and machine learning.</p>
<p>That all sounds like a lot of things! So this year is looking to be a busy one for me. However, I think that a lot of these goals are both interesting and necessary for my future plans, so I will try to take that motivation forward in pursuing these goals. Generally I am not one for clearly defined &rsquo;new years resolutions&rsquo;, because I try to improve myself at a slow and stedy pace, to increase the chance of making lasting changes and achieving long term goals. But I do think that it is good to take the time at the beginning of the year or even just the changing of the seasons to reflect on aspirations, goals, and so on. And there is no time like the last year of a PhD to reflect on these things!</p>
<p>Concretely, the priorities I have are focused around the completion of my PhD since I hope to do that with as few complications as possible, and my position currently affords a good opertunity for that. So my main focuses are towards the research most central to my PhD, as well as things that will be useful for my future as a researcher including relevant research for potential postdoc or other positions. Those specific goals definitely seem relatively focused, and I can always add on top of that some of the other ideas I have depending on whether I have the time for them.</p>
<p>Some of the personal aspirations I have other than research and job prospect related things are to read more fiction, as I do end up spending a good amount of time reading books and papers related to my research interests, and I&rsquo;d like to get back into my hobby of science-fiction and fantasy. I did get a great christmas gift of two books by JRR Tolkien that I am excited to read, and I am also in the middle of a fantastic high-fantasy book called &ldquo;The Priory of the Orange Tree&rdquo; by Samantha Shannon. I hope to read those and other books this year, as I have been enjoying slowly getting back into reading for fun. I think it is useful to put down my thoughts and aspirations in this blog as it allows me to reflect on them more than simply thinking about them to myself.</p>
]]></content></item></channel></rss>