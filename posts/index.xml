<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Posts on Human and Machine Learning in Cognitive Science</title><link>tylerjamesmalloy.github.io/posts/</link><description>Recent content in Posts on Human and Machine Learning in Cognitive Science</description><generator>Hugo -- gohugo.io</generator><language>en-us</language><copyright>&lt;a href="https://creativecommons.org/licenses/by-nc/4.0/" target="_blank" rel="noopener">CC BY-NC 4.0&lt;/a></copyright><lastBuildDate>Tue, 14 Feb 2023 00:00:00 +0000</lastBuildDate><atom:link href="tylerjamesmalloy.github.io/posts/index.xml" rel="self" type="application/rss+xml"/><item><title>Narrowing The Gap With LLMs</title><link>tylerjamesmalloy.github.io/posts/narrowing-gap/</link><pubDate>Tue, 14 Feb 2023 00:00:00 +0000</pubDate><guid>tylerjamesmalloy.github.io/posts/narrowing-gap/</guid><description>This will be my third blog post in a row (see the first, and second) on the topic of large language models. While this area of research has been in the news signifcantly recently, it is not exatly my area. However, there was a talk at the University of Pittsburgh philosophy department given by professor Colin Allen that was, at least partially, presented as a refutation to the position of David Chalmers that I discussed previously.</description><content type="html"><![CDATA[<p>This will be my third blog post in a row (see the <a href="https://tylerjamesmalloy.github.io/posts/sentience/">first</a>, and <a href="https://tylerjamesmalloy.github.io/posts/neurips-2022/">second</a>) on the topic of large language models. While this area of research has been in the news signifcantly recently, it is not exatly my area. However, there was a talk at the University of Pittsburgh philosophy department given by professor Colin Allen that was, at least partially, presented as a refutation to the position of David Chalmers that I discussed previously. Since I talked about Chalmers&rsquo; ideas in my previous blog post, I thought this would serve as a nice contrast to those ideas, and hopefully wrap up my thinking about it for the time being.</p>
<p>In Dr. Allen&rsquo;s talk, he discusses some issues with large language models and the continuing gap between AI and human intelligence. One of the arguments that Allen raises about the current issues with LLMs are the level to which they display &rsquo;thruthiness&rsquo;, a term coined by the late night comedian Stephen Colbert to refer to plausible sounding nonsense. This phenomenon can be seen on many examples of twitter posts and blog articles about the failures of chatGPT and related LLMs. In the talk, Allen presented his own exmaple where he repeatedly told chatGPT that its response was factually incorrect, which chatGPT agreed with, however it could not provide the correct response.</p>
<p>This over-agreeable behaviour is something I have noticed when using these LLM systems, that they rarely dsiplay their own preferences and refuse to take stances on many issues or opinions. Also, they often change their mind when presented with different points of view such as an argument I recently got into with chatGPT about the differences and similarities of the film Bladerunner and the book Do Android&rsquo;s Dream of Electric Sheep. During that discussion, chatGPT repeatedly mentioned that the movie is more &lsquo;visual&rsquo;. When I said that that distinciton isn&rsquo;t relevant when comparing a book and movie, chatGPT would initially agree with me before mentioning the same point again. Another fun problem for chatGPT is logic, which you can see an example of in a <a href="https://twitter.com/TylerJMalloy/status/1624277084544671744/photo/1">screenshot on my twitter</a>.</p>
<p>An interesting point that Dr. Allen raised is the &lsquo;dangers of averaging&rsquo;, which I have experienced in my own work in cognitive modelling. Here, when we try to predict how humans are learning in a task, we may take the average performance over some number of trials of a population of human participants, and try to build a model that reflects that average. However, if we were to then conclude that the humans and the cognitive model are behaving similarly, it would ignore the fact that there is typically a wide variation in performance of any task in a given population. Cognitive models should ideally reflect these realities, and try to explain what precicely is going on in each participant that lead them to their own level of performance. While this may not be a perfectly achievable goal, or necessary for all applications of cognitive models, it is important to remember that any system build on a large amoung of human generated data, like LLMs, can run into these types of issues.</p>
<p>Dr. Allen at one point summarized the position of David Chalmer&rsquo;s argument that LLMs, thought not currently sentient, could be added to in specific ways that would increase the liklihood that they would display some level of sentience. Allen diagreed with this general notion by arguing that what is truly missing from LLMs, that they can&rsquo;t solve with more layers or training, is an architecture that allows for meta-cognition. It is important, to Allen, that conscious agent&rsquo;s be able to &rsquo;take a step back&rsquo; and see why they are incorrect or correct using reasoning and logic. Currently logic problems and mathematics are big issues for vanilla LLMs, though they can be somewhat overcome by prompt engineering.</p>
<p>Personally, I agree in the importance of a meta-cognitive architecture that allows models to use different faculties to address a problem, and oversee the reasoning used to decide what faculty to use. However, the behaviour of LLMs have already demonstrated some phenomenon that philosophy of AI researchers previously have thought would require a specific architecture and training method, rather than the relatively simple end to end structure and training of LLMs. In Chalmer&rsquo;s talk, he noted the possibility that the easest way to train a model to do next in sentence word prediction may be to have the model learn a world representation that emerges through experience, an idea that was supported through recent studies of world representations in LLMs, mentioned in my last post. This could mean that the type of meta-cognitive architecture that Dr Allen argues for could theoretically emerge naturally though specific training methods and larger models. However, the end result may be further and further from how meta-cognition and logci is implemented in the human brain.</p>
<p>Ultimately, I am unsure whether or not is is possible for meta-cognition, logic, compositionality, or even consciousness to emerge from the type of training and structure that current LLMs are designed on. I think that this will be an important question in the next twenty years of AI research, and it is important not to dismiss out of hand arguments in either direction. This is an exciting area of potential research that will need to bring together philosophers of the mind and AI with engineers and companies that are interested in making their products as human friendly as possible.</p>
]]></content></item><item><title>Sentience and World Representations in LLMs</title><link>tylerjamesmalloy.github.io/posts/sentience/</link><pubDate>Tue, 24 Jan 2023 00:00:00 +0000</pubDate><guid>tylerjamesmalloy.github.io/posts/sentience/</guid><description>In my last blog post I discussed a few of the presentations given at NeurIPS 2022 that I found particularly interesting. I didn&amp;rsquo;t get a chance to write much on another presentation given by David Chalmers that was a condensed version of his earlier talk &amp;ldquo;Are Large Language Models Sentient?&amp;rdquo;. In this talk Chalmers discussed several possible positions on the question of sentience in large language models, systematically looking how these positions would define sentience and whether or not it is possible for LLMs like ChatGPT to exhibit those properties.</description><content type="html"><![CDATA[<p>In my last <a href="https://tylerjamesmalloy.github.io/posts/neurips-2022/">blog post</a> I discussed a few of the presentations given at NeurIPS 2022 that I found particularly interesting. I didn&rsquo;t get a chance to write much on another presentation given by David Chalmers that was a condensed version of his <a href="https://www.youtube.com/watch?v=-BcuCmf00_Y">earlier talk</a> &ldquo;Are Large Language Models Sentient?&rdquo;. In this talk Chalmers discussed several possible positions on the question of sentience in large language models, systematically looking how these positions would define sentience and whether or not it is possible for LLMs like ChatGPT to exhibit those properties.</p>
<p>One of the commonly quoted properties that you might require for sentience is a world model, some internal representation of the state of the world that an experiencing agent exists in. An interesting idea that was discussed by Chalmers in this talk is that, while LLMs are trained to do next word in sentence prediction, it could theoretically be the case that the best way to learn how to do that is to have a world representation that might emerge out of training these large language models.</p>
<p>A <a href="https://openreview.net/forum?id=DeG07_TcZvT">recent paper on openreview</a> for ICLR 2023 titled &ldquo;Emergent World Representations: Exploring a Sequence Model Trained on a Synthetic Task&rdquo; explores this idea of emergent phenomenon on large language models. This paper explores the question of whether or not LLMs  &ldquo;just memorize a collection of surface statistics, or do they rely on internal representations of the process that generates the sequences they see?&rdquo;. The conclusions from these experiments, using the board game othello as an environment, are that specifically trained LLMs might exhibhit an emergent property similar to a world representation that is often said to be lacking from LLMs.</p>
<p>This Chalmers talk also discusses possible additions onto LLMs (which he termed LLM+ models) that include other domains like images, videos, reinforcement learning and even theoretically other types of perception. The goal of the talk was to suggest that, even for some of the more strict definitions of what is required for sentience, we could imagine how such a LLM+ model would be constructed and trained.</p>
<p>So where does my research experience connect with these ideas? Typically we think of these world representations as being severely constrained in humans, and this could be a big difference between emergent world models in humans and  LLM+, such as the <a href="https://www.theverge.com/23560328/openai-gpt-4-rumor-release-date-sam-altman-interview">much rumored about GPT4 model</a>. A big part of my dissertation was trying to understand how the brain makes optimal representations of visual information in light of both information constraints and behavioral goals. However, if models are trained in an unconstrained manner to optimize next word prediction using billions or trillions of parameters, then any emergent phenomenon, either world models or other types of phenomenon like consciousness, might have significantly different properties than biological phenomenon.</p>
<p>While that position may sound like it is assuming that consciousness is an emergent phenomenon in humans, I don&rsquo;t want to come across as saying that is definitely the case. While it might or might not be true, I would agree on the implications of the nature of sentience or world models as they might be emerging in LLMs and other types of artificial intelligence. Specifically, if the training and optimization of these models does not taking into account information constraints, then it would likely be different from human cognition.</p>
<p>One of the possible differences exists in the example given in the &lsquo;Emergent World Representations&rsquo; paper, which used Othello games as examples for testing whether a model has a representation of the state of an Othello game. Even with this simple board game example, we can start to think of how an unconstrained LLM might represent the state of an othello game differently than the human mind. One area where the model is less constrained is in the informational content of the representation, which we can assume is in the form of textual information. Comparing this to the human mind, we might expect that some of the representation of the world state of the game is in visual information, but also that this representation could be turned into a linguistic description of the pieces on the board.</p>
<p>This difference in representations might be solved by a LLM+ type model that also includes multimodal perception like images or videos and sound. However, it might be that even that type of model would still represent the world state significantly differently due to the lack of the same types of constraints that exist in the human mind. It will be interesting to see and compare the behaviour of the next generation of LLMs and other models with what we expect and know of the human mind and consciousness.</p>
]]></content></item><item><title>NeurIPS 2022</title><link>tylerjamesmalloy.github.io/posts/neurips-2022/</link><pubDate>Mon, 05 Dec 2022 00:00:00 +0000</pubDate><guid>tylerjamesmalloy.github.io/posts/neurips-2022/</guid><description>This year I attended the Conference on Neural Information Processing Systems for the first time to present a poster alongside a paper that was accepted at the workshop on Information Theoretic Principles in Cognitive Systems.
The first day fo the conference I attended the NewInML workshop, which was intended for researchers that are new to the machine learning community. Since this was my first NeurIPS and my background is in cognitive science, this felt like the perfect way to start the conference.</description><content type="html"><![CDATA[<p>This year I attended the Conference on Neural Information Processing Systems for the first time to present a poster alongside a paper that was accepted at the workshop on Information Theoretic Principles in Cognitive Systems.</p>
<p>The first day fo the conference I attended the NewInML workshop, which was intended for researchers that are new to the machine learning community. Since this was my first NeurIPS and my background is in cognitive science, this felt like the perfect way to start the conference. The topics of this workshope varried from general overviews of researchers experiences in machine learning, to more specific topics like navigating the early years of a tenure-track position or how to best negotiate a machine learning engineer job in industry.</p>
<p>I was glad there was a wide variety of topics as it kept the long day of talks interesting, but my favorite presentation was the first of the day given by Yoshua Bengio. This consisted of a broad overview of the history of his research, including the early days of research into neural networks before the so-called AI winter. It was interesting to hear Yoshua talk about perspectives on these famous epochs in AI research and what kept him going and interested in different areas that eventually lead to some of the impressive systems we have today. While it is hard to predict the future, if I am to stay in machine learning research for a while then it could probably be expected that the field will experience something along the lines of the AI winters of the past, and staying interested in my work will be an important skill.</p>
<p>There were several interesting invited talks given during the main conference, one of my favorites was given by Geoffrey Hinton titled &ldquo;The Forward-Forward Algorithm for Training Deep Neural Networks&rdquo;. This talk was mainly focused on discussing a more biologically plausable method of training neural networks as an alternative to the backpropegation method currently in use. Hinton highlighted several issues with the biological plausibility of backpropegation, and introduced what he believes is a more biologically plausible network training method, called the feed-forward algorithm. There is an acompanying paper on Hinton&rsquo;s website that explains in detail the motivation, methods, and some preliminary experimentation on simple domains.</p>
<p>While the algorithm introduced in the talk is titled &lsquo;feed-forward&rsquo;, Hinton does not suggest that the brain is entirely feed-forward, as it is highly recurrent, but rather that it can be trained in that directional dependency, as opposed to backpropogation which lacks a biological basis. The main thrust of the model seemed to me to be similarly motivated as the free-energy principle, by relying on a model that determines the probability that the current stimuli is in a sense accurate, through the introduction of an idea of a &lsquo;goodness&rsquo; measure. This can be connected to other topics like the purpose of sleep, which Hinton suggests is both a way to train the brain to be able to generate its own experience, useful for real planning, and also as a discriminator for better labelling unreal experience.</p>
<p>One interesting point from this talk is highlighting the scale of the human cortex which he lists as roughly 10^14 connections, with &lsquo;only&rsquo; 10^9 seconds to train them, and that &ldquo;It is possible that the learning algorithm we have is not so good (as ANNs) at squeezing a lot of knowledge into a few weights, it has the opposite problem&hellip; we are data limited rather than capacity limited&rdquo;. I thought that this was an interesting point to bring up in a talk on a more biologically plausible neural networks, since most of my work on making machine learning techniques more human-like has been focused on explicitly modelling the limitations of human learning. In fact, Hinton uses the term &ldquo;Capacity-Limited&rdquo;, which has directly inspired my work in making reinforcement learning models that are closer to human like learning. However, I don&rsquo;t see the main point of this talk as totally disconnected from my own work, since it broadly is interested in more human like machine learning, while it takes a different perspective on better understanding now NNs can be trained, and my work focuses more on how humans leverage experience and biases to learn quickly, but in ways that can lead to suboptimal behaviour.</p>
<p>The workshop on Information Theoretic Princples in Cognitive Systems was created by, among others, Sam Gershman and Irina Higgins, both of whom I have cited extensively and take great inspiration from. My paper for this workshop was titled <a href="https://www.researchgate.net/publication/365203245_Learning_in_Factored_Domains_with_Information-Constrained_Visual_Representations">Learning in Factored Domains with Information-Constrained Visual Representations</a>. This paper is an extenstion of the model that I developed for my dissertation, and the human experiment that was run alongside that. The main differences with the approach used in this workshop paper is that the model uses latent representations of visual information as the input to a hypothesis generation and evaluation method, as opposed to a seperate neural network trained to predict utility. This allowed for extremely fast learning in a contextual bandit setting based on images of human faces, with the model selecting the correct option after only 2-3 experiences in the task despite the highly complex state information. I am exited to see where this research can extend to, potentially in designing a new experiment for human learning in the type of task described in this paper, as I think the proposed model would be a great explanation for human learning while also making predictions of how humans represent visual information during learning.</p>
]]></content></item><item><title>PhD Defense</title><link>tylerjamesmalloy.github.io/posts/phd-defense/</link><pubDate>Thu, 10 Nov 2022 00:00:00 +0000</pubDate><guid>tylerjamesmalloy.github.io/posts/phd-defense/</guid><description>It has been a while since I made a blog post, mostly becase I have been working hard on my PhD Dissertation and practicing for my defense, which you can watch a practice run through of at this link. Since the last time I made a post I have defended my dissertation and I am now in the final stages of my work at Rensselaer. I have had an amazing time working at RPI with my mentor Chris R.</description><content type="html"><![CDATA[<p>It has been a while since I made a blog post, mostly becase I have been working hard on my PhD Dissertation and practicing for my defense, <a href="https://youtu.be/Y9itvE0H7-Y">which you can watch a practice run through of at this link</a>. Since the last time I made a post I have defended my dissertation and I am now in the final stages of my work at Rensselaer. I have had an amazing time working at RPI with my mentor Chris R. Sims, as well as working in collaboration with the Reinforcement Learning research group at IBM, particularly with my mentor Tim Klinger. The next big step for me is starting my postdoctoral fellowship research position working with Coty Gonzalez at Carnegie Mellon in December of this year.</p>
<p>So onto the dissertation defense itself. I had practiced the presentation around a dozen or so times in the leadup to the day of the defense, and I felt that I had I good handle on the flow of the presentation. One thing that was difficult, that I assume many PhD defense presenters find, is that discussing a 100 page dissertation in 45 minutes can be very challangeing. I was lucky partially in that I had given an approximately 30 minute presentation to the Cognitive Science department at RPI only a few weeks prior, as a part of a yearly requrement for graduate students in the department.</p>
<p>That earlier presentation ended up being a preview of what I would discuss in my dissertation defense, and was very helpful in finding out what I needed to describe to someone who was unfamiliar with my disseration topic. Typically for a broad audience, like a university department, it makes sense to eschew any highly technical aspects such as loss functions used for training, or algorithms that define utility in decision making. This was my approach to the earlier introductory presentation, and I felt that a high level description of the real-world psychological phenomenon I was interested in was useful for that.</p>
<p>When it came time to begin putting together my dissertation defense, I started with this preliminary presentation, and added on any necessary detail that I would think someone in my dissertation committee, or another researcher familiar with this area, would be wondering. For me specifically, this was information on how my model was trained to predict human behaviour, and how the comparison models made their own predictions of human behaviour. This allowed me to compare in more detail my proposed model and related methods for explaining behaviour, to better explain where the differences were coming from.</p>
<p>The most significant aspect of my model that I was able to explain in more detail in the longer version of my presentation was that it is able to reflect biases present in human decision making as a result of information constraints, and not through the intoduction of additional parameters that alternative models rely on. I think that this is the easiest to understand benefit of my proposed model, and I was happy that I got to explain it in my dissertation defense. Though in order to quantify that difference, I needed to explain the 2 alternative versions of predicting human behaviour, one that does so without explicitly modelling the observed bias and one that does, and this was required for the two types of task in my experiment, as well as 2 alternative deep neural network based methods for a total of 6 comparison models.</p>
<p>Having this much more to discuss and only a slightly longer presentation time meant cutting or summarizing a lot of the content from my earlier presentation, but deciding what to cut and what to keep was easier than expected. There were a few long descriptions of real-world examples in my first presentaiton that I significantly summarized. Additionally, explaining the background of the model I proposed was shortened, given that the main audience would have (hopefully) read my dissertation. None of these things should totally be removed from an ideal dissertation defense, though explaining everything for a general audience is also not totally necessary.</p>
<p>With those changes to the presentation, and the previously mentioned dozen or so practice runs, I was able to succesfully defend my dissertation with a few minor revisons requested by my committee. During the presentation itself there were some clarifying questions from my committee that weren&rsquo;t too difficult to adreess. Afterwards the only question form the general audience came from my friend who works in vision science, wondering about the broad implications of my work for her field. I would have liked to make some strong claims about visual representations, but was more focused on learning and didn&rsquo;t want to make any claims not fully backed up by the experiment I presented.</p>
<p>After a short deliberation period I was welcomed back into the room by my advisor Chris, who had a smile on his face as he said the words I had been waiting nearly 5 years to hear &ldquo;Congratulations Dr. Malloy&rdquo;.</p>
]]></content></item><item><title>What's in a Representation?</title><link>tylerjamesmalloy.github.io/posts/whats-in-a-representation/</link><pubDate>Mon, 29 Aug 2022 00:00:00 +0000</pubDate><guid>tylerjamesmalloy.github.io/posts/whats-in-a-representation/</guid><description>As I begin the final stages of my PhD thesis, I was recently surprised by one topic that jumped out as being more relevant than I had originally thought of when I began work on the project. Broadly, the topic is in the realm of cognitive modelling, specifcially modelling and predicting the behaviour of humans performing a learning and decision making task based on visual information. Traditional approaches to predicting how humans learn and make desicions have done so by abstracting away much of the compelxity of the task presented to humans, and modelling their behaviour as some simple function of the features of the task, such as a soft-max distribution over the utilities associated with the options presented.</description><content type="html"><![CDATA[<p>As I begin the final stages of my PhD thesis, I was recently surprised by one topic that jumped out as being more relevant than I had originally thought of when I began work on the project. Broadly, the topic is in the realm of cognitive modelling, specifcially modelling and predicting the behaviour of humans performing a learning and decision making task based on visual information. Traditional approaches to predicting how humans learn and make desicions have done so by abstracting away much of the compelxity of the task presented to humans, and modelling their behaviour as some simple function of the features of the task, such as a soft-max distribution over the utilities associated with the options presented.</p>
<p>For context, when I began my PhD I was interested in two main areas, cognitive modelling and deep learning, which I had hoped to connect together throughout the work that I did during my time in grad school. This was not a wholely new approach to modelling human behaviour in complex tasks. However, the choice of how best to model cognition should not be motivated by individual interests alone. To motivate the use of deep learning approaches in modelling human cognition required a conectinon to biological percpetion, specifically vision.</p>
<p>One commonly cited and long-standing connection between human visual perception and deep learning is between the activation of convolutional neural networks and the preliminary processing of visual information that takes place in biological brains. However, recent research by Higgins et al. has investigated how visual information is represented in highly specialized areas of the brain, such as the primate face area, which processes visual information related to primate faces. Results from analysis of the activation of single neurons in the primate face area revealed a so-called disentangled property, meaning that deviation along a single dimension of input stimuli like chaning the age alone or hair color alone resulted in an altered activity of individual neurons. This is closely connected to the behaviour of beta-Variational Autoencoders which are trained to form low-dimension representations of stimuli that are information-constrained and able to reconstruct the original stimuli as accurately as possible.</p>
<p>The similarity of this highly specialized brain region to the properties of B-VAE models raises an important question of the structure of information as it is being processed by the reinforcement learning faculty of the human brain. Specifically the midbrain of humans has long been analyzed and understood as the centre of RL in biological agents, with dopamine acitivty serving as an alogue to the reward prediction error used in RL to update the predicted values associated with certin stimuli. To properly function, RL must make predictions of reward associated with stimuli, or states of an environment, and update these predictions based on experience.</p>
<p>In Deep RL this is typcially done using a CNN or other deep neural network model that processes the visual information to form a state representation, and predictions of reward are made based on these representations. However, there are many issues with applying Deep RL methods that use CNNs to predicting human behaviour, and much of the research I have done so far in my PhD has been in trying to improve the generalization and robustness used in deep RL methods when predicting human behaviour. For my final thesis project, I will be applying B-VAE models onto predicting learning and decision making in humans based on visual information. The main thrust of this research is in analyzing the representations formed by Deep RL models when they are trained to predicut human behaviour. Hopefully, this analysis will reveal the source of some common biases in human learning and decision making, as well as the good qualities of human learning such as robustness and generalization.</p>
<p>To draw this research back to the title of this blog post, I am interested in investigating what information is contained within cognitive neural representations as that information flows from the visual information processing regions of the brain into the reinfocement learning regions.</p>
<p>To hear more about this project, stay tuned for an update on my PhD thesis towards the end of the year!</p>
]]></content></item><item><title>Vss 2022</title><link>tylerjamesmalloy.github.io/posts/vss-2022/</link><pubDate>Sat, 28 May 2022 00:00:00 +0000</pubDate><guid>tylerjamesmalloy.github.io/posts/vss-2022/</guid><description>Earlier this month I attented the Visual Science Society 2022 conference where I gave a talk titled &amp;ldquo;A Beta-Variational Auto-Encoder Model of Human Visual Representation Formation in Utility-Based Learning&amp;rdquo;. The entire abstract I submitted is copied at the bottom of this blog post if you would like to read it.
The VSS experience was very positive, I was somewhat worried or hesitant that I would not get a lot of the posters and talks since my experience is much more in the learning domain rather than vision.</description><content type="html"><![CDATA[<p>Earlier this month I attented the Visual Science Society 2022 conference where I gave a talk titled &ldquo;A Beta-Variational Auto-Encoder Model of Human Visual Representation Formation in Utility-Based Learning&rdquo;. The entire abstract I submitted is copied at the bottom of this blog post if you would like to read it.</p>
<p>The VSS experience was very positive, I was somewhat worried or hesitant that I would not get a lot of the posters and talks since my experience is much more in the learning domain rather than vision. While there were many talks and posters that I didn&rsquo;t fully understand, all of the participants and presenters made a good effort to explain things for a general audience and I didn&rsquo;t have too much trouble getting the general gist of projects.</p>
<p>Apart from the general vision research, there was, to me, a suprising number of Deep Neural Network techniques used in various vision related projects, as well as a fair number of tasks that involved either learning or decision making, which is the focus of my thesis. I was able to meet with a few PIs who were presenting posters related to their work that I was very interested in and discuss how my work relates to there.</p>
<p>Another good experience was the industry meeting. While I do have experience in industry research somewhat through my internship with IBM, it was interesting to see how different companies handle AI and Human learning research. There was a good combination of bigger companies (Apple and Google), as well as smaller companies and start ups that have very different structures when it comes to research.</p>
<p>The other great aspect of VSS is of course the location, St.Pete&rsquo;s beach Florida which had amazing weather. I had heard that in previous years the weather had been very hot and somewhat muggy/humid. This year there was probably only one day where I felt too hot/humid while walking around. I did get to stay in the same hotel where the conference was so I was able to pop back to my room if things got too sweaty, which I would recommend for anyone able to when attending either VSS or another conference in a hot area in the summer. So all in all I had a great experience and I&rsquo;m excited to head to my next conference Reinforcement Learning and Decision Making next month!</p>
<p>Here is the link for the <a href="https://www.youtube.com/watch?v=g0KcUqXB43U" title="VSS Presentation"><img src="https://www.youtube.com/watch?v=g0KcUqXB43U0.jpg" alt="VSS Presentation"></a> for just my talk, for you to view.</p>
<p>Abstract:
Tyler Malloy1 (<a href="mailto:mallot@rpi.edu">mallot@rpi.edu</a>), Chris R. Sims; Rensselaer Polytechnic Institute</p>
<p>The human brain is capable of forming informationally constrained representations of complex visual stimuli in order to achieve its behavioural goals, such as utility-based learning. Recently, methods borrowed from machine learning have demonstrated a close connection between the mechanisms of visual representation formation in primate brains with the latent representations formed by Beta-Variational Auto-Encoders (Beta-VAEs). While auto-encoder models capture some aspects of visual representations, they fail to explain how visual representations are adapted in a task-directed manner. We developed a model of visual representation formation in learning environments based on a modified Beta-VAE model that simultaneously learns the task-specific utility of visual information. We hypothesized that humans update their visual representations as they learn which visual features are associated with utility in learning tasks. To test this hypothesis, we applied the proposed model onto the data from a visual contextual bandit learning task [Niv et al. 2015; J. Neuroscience]. The experiment involved humans (N=22) learning the utility associated with 9 possible visual features (3 colors, shapes or textures). Critically, our model takes in as input the same visual information that is presented to participants, instead of the hand-crafted features typically used to model human learning. A comparison of predictive accuracy between our proposed model and models using hand-crafted features demonstrated a similar correlation to human learning. These results show that representations formed by our Beta-VAE based model can predict human learning from complex visual information. Additionally, our proposed model makes predictions of how visual representations adapt during human learning in a utility-based task. Further, we performed a comparison of our proposed model across a range of parameters such as information-constraint, utility-weight, and number of training steps between predictions. Results from this comparison give insight into how the human brain adjusts its visual representation formation during learning.</p>
<p>Acknowledgements: This work was supported by NSF research grant DRL-1915874 to CRS and an IBM AIRC scholarship to TJM</p>
]]></content></item><item><title>Beyond Reward</title><link>tylerjamesmalloy.github.io/posts/beyond-reward/</link><pubDate>Thu, 07 Apr 2022 00:00:00 +0000</pubDate><guid>tylerjamesmalloy.github.io/posts/beyond-reward/</guid><description>In a previous post I mentioned an interesting paper that made the claim that much of human intelligence could be viewed under the lense of reward maximization, you can see that blog post here. This point of view may not be the most common among either psychologists or computer scientists, but it would be great news for reinforcement learning researchers who are interested in making very smart systems by training them to maximize reward.</description><content type="html"><![CDATA[<p>In a previous post I mentioned an interesting paper that made the claim that much of human intelligence could be viewed under the lense of reward maximization, you can see <a href="https://tylerjamesmalloy.github.io/ty-malloy/review/2022/01/17/Reward-Review.html">that blog post here.</a> This point of view may not be the most common among either psychologists or computer scientists, but it would be great news for reinforcement learning researchers who are interested in making very smart systems by training them to maximize reward. However, I previously discussed the training aspect of reward maximization and what I believe it can achieve. Instead of discussing the ways that we can train artificial intelligence, specifically reinforcement learning agents, in this blog post I am going to talk about the assessment of RL agents.</p>
<p>The most common way to assess the performance of RL agents is their episodic or long-term reward, usually displayed as a learning curve as the training of the agent progresses and compared against alternative RL models in a given environment. This differs from other ML techniques like classification or regression problems, two of the main applications of ML, which are typically quantified in terms of error or difference between what the model predicts and the correct solution. This is in part due to the unique nature of RL among ML and AI techniques as a semi-supervised method. We could theoretically graph the error of the NN being trained in either the Q-network or policy network that instantiates our RL agent, but that may not be as informative as the reward. For many applications reward maximization is a good metric for performance, such as playing chess or go, or controlling the electrical output of a factory, etc.</p>
<p>However, if we are interested in human-like reinforcement learning, as I am, then reward maximization may not be the best or even a particularly useful metric to judge how well we are achieving our goals. TO use a specific example I will talk about the multi-agent particle environment suite, which was used in one of my previous papers that tried to apply human-like learning onto RL agents. Those environments are shown in Figure 1 and a citation to both my paper as well as the original paper that described them is listed in the reference section. These are very interesting environments from the perspective of designing human-like RL since they require some aspect of multi-agent interaction consisting of a mix of communication in cooperative, competitive, and mixed environments. These types of communicative tasks may be simple while simultaneously revealing many important and interesting aspects of human behaviour.</p>
<!-- raw HTML omitted -->
<p><!-- raw HTML omitted --><!-- raw HTML omitted --><!-- raw HTML omitted -->Figure from my paper [2] describing the Multiagent Particle Environments [2]. Adversary: 2 Good agents and 1 adversary are rewarded by closeness to a target, good agents must not reveal which object is the target by spreading to both the target and distraction. Crypto: 1 Good agent communicates target landmarks to another good agent over a public communication channel, 1 adversary attempts to decode the communicated target. Push: 1 good agent moves towards the target landmark while avoiding 1 adversary. Reference: 2 mobile good agents communicate to determine which landmark is the target. Speaker: 1 static good agent communicates to 1 mobile good agent which landmark is the target. Spread: 3 good agents spread to cover all landmarks. Tag: 1 good agent moves to distance itself from 3 adversaries using obstacles to slow their approach. World: 2 good agents move to gather food, hide within trees, and avoid 4 adversaries, 1 adversary leader can observe good agents hiding in trees and communicate their location.<!-- raw HTML omitted --><!-- raw HTML omitted --></p>
<!-- raw HTML omitted -->
<p>In this paper, we measured the performance in the same way that many papers that utilized this suite of environments had, by taking two models and comparing the performance against each other. In cooperative games all agents are the same model, and in competitive games one team of agents is the proposed model with the other being controlled by some baseline. Results from this direct reward comparison showed some modest improvements in reward, especially in the competitive tasks which amplified any differences in ability between the two agents. However, these types of modest results are often thought of by reviewers as within the bound of what you might expect by performing some hyper-parameter optimization on a given model, and thus not typically enough to claim strongly that one model is superior to another. While true, I think that these types of assessments partially miss the point of human-like RL in general, which seeks to make the performance of RL agents more human-like where possible. Though it is understandable that one would come to this conclusion given the way that we tried to compare performance in the paper.</p>
<!-- raw HTML omitted -->
<p><!-- raw HTML omitted --><!-- raw HTML omitted --><!-- raw HTML omitted -->Reward results in MPE suite comparing our information-constrained model with a baseline in the final 1K episodes of training.<!-- raw HTML omitted --><!-- raw HTML omitted --></p>
<!-- raw HTML omitted -->
<p>Instead of continuing to use averaged rewards from thousands of randomized trials, I believe it may be more useful and perhaps necessary for the evaluation of human-like behaviour in environments that require complex social interactions and communication. Take for example the adversary environment which requires the two good agents to split up between the target location and the distractor, to prevent the bad agent from determining which is the target. If we imagine a constructed beginning state with deterministic good agents that always move towards the target for a bit before splitting up, it should be easy for the bad agent to determine which is the area of interest. However, the model I proposed in the previous paper would have no way to directly determine this based on the previously revealed goals and behaviour of these good agents. Thus this type of constructed example could be perfect to assess human-like psychologizing in our agents.</p>
<p>I hope to use these ideas as I begin my work on card games that require theory-of-mind reasoning like poker, hanabi, mahjong, and bridge. All of these are great and interesting domains that allow for the analysis of complex psychological phenomena, but simply averaging over thousands or even millions of trials may result in average rewards that suggest different models are more similar than they truly are. If we were to construct specific environment states and opponent or team mate behaviours then we could hopefully get a more fruitful assessment of performance that doesn&rsquo;t miss the forest for the trees.</p>
<h2 id="references">References</h2>
<p>[1] T. Malloy, C. R. Sims, T. Klinger, M. Liu, M. Riemer and G. Tesauro, &ldquo;Capacity-Limited Decentralized Actor-Critic for Multi-Agent Games,&rdquo; 2021 IEEE Conference on Games (CoG), 2021, pp. 1-8, doi: 10.1109/CoG52621.2021.9619081.</p>
<p>[2] R. Lowe, Y. Wu, A. Tamar, J. Harb, P. Abbeel, and I. Mordatch, “Multi-agent actor-critic for mixed cooperative-competitive environments,” in Proceedings of the 31st International Conference on Neural Information Processing Systems, 2017, pp. 6382–6393.</p>
]]></content></item><item><title>Rl Web Security</title><link>tylerjamesmalloy.github.io/posts/rl-web-security/</link><pubDate>Sat, 05 Mar 2022 00:00:00 +0000</pubDate><guid>tylerjamesmalloy.github.io/posts/rl-web-security/</guid><description>Since my partner is a web security expert, I often end up having long discussions about internet security, even though my personal knowledge and research is in a very different area. That has recently gotten us to thinking about the intersection of reinforcement learning and web security. Though at first these may seem like two disparate areas, as anyone who has had experience talking at length with another person about their specific research area will know, eventually there are many commonalities that can be found between the two.</description><content type="html"><![CDATA[<p>Since my partner is a web security expert, I often end up having long discussions about internet security, even though my personal knowledge and research is in a very different area. That has recently gotten us to thinking about the intersection of reinforcement learning and web security. Though at first these may seem like two disparate areas, as anyone who has had experience talking at length with another person about their specific research area will know, eventually there are many commonalities that can be found between the two.</p>
<p>These connections were relatively easy to make since my own personal experience in reinforcement learning research is in the area of human-inspired RL. To the best of my understanding, web security revolves around the ways that humans and automated systems interact with web based services. Because of this, RL agents designed with a human-centric approach seem relatively well suited for interacting with these systems.</p>
<p>The first use case for such RL systems that immediately jumps to mind is in performing some type of testing for existing web services or ones in development. I do have some minor experience in web application software testing, though it was in performance specifically and centered moreso around timing and user satisfaction rather than data security.</p>
<p>As with all potential use-cases of RL it is a fun mental exercise to break down the state, environments, actions, and rewards to see if we can immediately make sense of what the system would look like and how it would perform its task. Additionally this is useful in determining if existing solutions are sufficient or better suited to the tasks that are in question. We can use the classic diagram from Sutton and Barto as a reference for the interaction between these parts.</p>
<!-- raw HTML omitted -->
<p><!-- raw HTML omitted --><!-- raw HTML omitted --><!-- raw HTML omitted -->Fig.1 - Reinforcement Learning Environment Diagram, from Sutton and Barto RL, hosted by lcalem on github .<!-- raw HTML omitted --><!-- raw HTML omitted --></p>
<!-- raw HTML omitted -->
<p>For the environment, we would likely want to limit our area of investigation to a particular website or web based application/system to limit the space we are interested in. However, there are many possible structures for how information is represented on the web, and we should additionally limit, at least for our brief discussion, how this information is represented. One particularly useful manner is provided by the REST API or Representational State Transfer Application Programing Interface. Even without knowing much about it, this seems like a great way to structure our RL system, State (the focus of all RL systems) is right in the name!</p>
<!-- raw HTML omitted -->
<p><!-- raw HTML omitted --><!-- raw HTML omitted --><!-- raw HTML omitted -->Fig.1 - REST API diagram, from astera. .<!-- raw HTML omitted --><!-- raw HTML omitted --></p>
<!-- raw HTML omitted -->
<p>Not only does the REST API provide a manner for representing data, more importantly it describes how the user, in this case our RL algorithm, interacts with a web based system. Just looking at the two diagrams shown previously we can easily draw direct connections between their functionalities.</p>
<p>Generally the information contained in a REST API based response from a web server is contained in a specific format such as HTML, XML, or JSON depending on the application. If we were to build a real RL system we would likely again want to narrow down (you are likely starting to see a trend) the scope by selecting one of these formats. Alternatively we could try training a more general RL system with either a logic based preprocessing step unique to each of the representation methods, or some other information structuring approach.</p>
<p>Although it may seem like an implementation level or ML engineering based question of how exactly this information is processed prior to being inputted into the RL system, much of the current research around logical based AI, neuro-symbolic AI, fuzzy logic, logic nets and so on seems highly relevant in the case of the environment we are interested in. All of those domains would probably have a useful insight into how the information should be structured and manipulated to best replicate something like general human behavior when interacting with a web based system. However, which and how those domains impact our RL system would depend on what exactly the goals are, and while it is interesting I should limit the scope (pun intended) of this blog post and move onto the reward function.</p>
<p>Now that we have a rough sketch for our environment, actions, and how both of those are going to be represented, we can begin looking at the reward and how we would train such a system. As I mentioned at the beginning, I am most experienced in building RL systems that in some way replicate how a human might interact with a similar system, given their limited attention and cognitive resources. With our current example, this might seem like a good application in inverse reinforcement learning, using human or expert behavior as a basis for training our RL algorithm. That might be a good approach if we are interested in creating the same effect as ‘smart scripts’ that could be used to stress test a system in a similar manner to real human users.</p>
<p>Another approach that is very different is more in the domain of what initially motivated this blog post, in the area of web security. Here we have a similar environment and action space, but the goal is very different. Instead of just replicating general human interaction with a web system, we want to answer the question of how secure our system is. As before one idea immediately jumps to mind, provided by the structure of web-based CTF (capture the flag) challenges which consist of a human interacting with some web system with the goal of collecting some hidden information (the flag).</p>
<p>This type of expert level behavior may require far too much generalization to be trainable from human experience, even with all of the advances of language models and other approaches that leverage massive datasets related to a specific task. Though it does seem like the type of thing we could expect web security researchers and experts to be using as a tool in the not too distant future. Since my own experience is less on the side of web security I won’t get too much into the details for fear of getting something wrong, though I likely have made some mistakes already. But nonetheless this does seem like an interesting application to me that I’ll continue to think about as my research continues.</p>
]]></content></item><item><title>Wordl Rl</title><link>tylerjamesmalloy.github.io/posts/wordl-rl/</link><pubDate>Thu, 03 Feb 2022 00:00:00 +0000</pubDate><guid>tylerjamesmalloy.github.io/posts/wordl-rl/</guid><description>This post will discuss briefly the possiblity of constucting a reinforcement learning algorithm to play the game Wordle. Language based applications of reinforcement learning are somewhat common, though perhaps not the first thing RL researchers think of as examples of applications in RL. However, Wordle is a single player game with a discrete number of actions and states, the proverbial bread and butter of RL algorithms such as one of the first successful game players TD-gammon, which palyed backgammon.</description><content type="html"><![CDATA[<p>This post will discuss briefly the possiblity of constucting a reinforcement learning algorithm to play the game Wordle. Language based applications of reinforcement learning are somewhat common, though perhaps not the first thing RL researchers think of as examples of applications in RL. However, Wordle is a single player game with a discrete number of actions and states, the proverbial bread and butter of RL algorithms such as one of the first successful game players TD-gammon, which palyed backgammon.</p>
<p>So it seems like RL may be a good idea for a AI mehtod, but how to acheve that isn&rsquo;t totally clear. I won&rsquo;t be discussing a clear implementation or writing any code in this post, just discussing the possibility. Before describing the game it should be noted that wordle is definitely possible to learn using RL, but what isn&rsquo;t clear is whether it is a good idea to do it or if there is a much better approach without RL.</p>
<p>Wordle is a simple word guessing game with the following instructions:</p>
<p>Guess the WORDLE in 6 tries.</p>
<p>Each guess must be a valid 5 letter word. Hit the enter button to submit.</p>
<p>After each guess, the color of the tiles will change to show how close your guess was to the word.</p>
<p>There are some interesting additional rules that are not included in this list of instructions. Firstly, the exact colors that the tiles change to and their meaning. A green highlighted color means that the letter you guessed is correct and in the correct location. Yellow means that the letter is correct but in the wrong location. And grey means that the letter does not appear in the target word. This is important from a reinforcement learning perspective as the state used to determine the optimal action should ideally contain this information.</p>
<p>These color codes are important for the game from a reinforcement learning perspective, but there are additional rules that limit what can be entered by the player. Firstly, words entered need to be in the valid list of words, more on that later as it is important when thinking about training a RL agent. Secondly, letters that have previously been shown to not be a member of the list (grayed out) are possible to be entered in. This is interesting as it allows for technically suboptimal behaviour, but this makes sense given the game is designed for humans who may be trying to enter one of the only words they can think of to see if any of the letters in it match and gain more inforamtion. We are only just describing how the game works in a basic sense and we are already talking about the difference between a human and AI player! Clearly this was destined as I am the one writing it, but I thought that was interesting nonetheless.</p>
<p>So if we were to design an agent we would definitelty want to be able to train it not just on the relatively short list of previous wordle answers, especially since those are unlikely to be repeated. There doesn&rsquo;t seem to be an official list of 5 letter words that are used in wordle, but there is a list of a high number of 5 letter english words, which may be the best bet as dictionaries change from year to year. <a href="https://eslforums.com/5-letter-words/">An incomplete list can be found here.</a> So with that the RL agent seems relativly straightforward right? We have our observation space and a way to get new episodes for training from that long list. But wait, what exatly is our action space?</p>
<p>If you know a bit about RL you may have an idea, but it may not be the one someone else is thinking of as there are, at least to me, two clear possibilities. Firstly the action space could be one of the roughly three thousand words in that long list I mentioned earlier. That may seem like a lot but there are domains I have worked with that had about that many actions, and other domains have far more or continuous actions which are a whole different issue. An alternative to this would be treating an action as entering a single letter. That may seem like a huge improvement in the complexity of the task, 3K to only 26! However remember that the entire word needs to be entered. So really to enter one word there are 26^5=11881376 different action combintaitons. This isn&rsquo;t as big an issue as it seems becasuse the state will be changing as the actions are made, the agent isn&rsquo;t required to pick the combination before they start typing. However from this we can see that the choice of action representation is not directly clear.</p>
<p>Since I am interested in human learning and machine learning, specifically in human inspired machine learning, it is interesting to think about how a human inspired RL method would tackle this problem. It may seem like humans definitely pick a word first and then type it out, giving credence to the worde based action space method. However, someone who has read strategies on wordle may recognize the strategy of copying down the green letters and putting X&rsquo;s in all the other spaces to allow you to visualize what the correct letter is. This method may suggest that people start typing out the letters they know, either at the beginning or end of the word, and then try to think of what the word might be.</p>
<p>There are many other topics that are interesting to think about from a human learning perpective, like trying to do imitation learning on examples from human players. There are also additionaly little quirks of the game like knowing how many of a letter there are based on the number of yellow highlighted letters in a word that had two of those letters in it. As a whole, I think this may be an example where RL might not be the best approach, as it is possible to simply have a list of all 5 letter words and remove the ones that don&rsquo;t fit the known information. This alone won&rsquo;t make a optimal player as they should additionally pick options that are maximally informative, but that could be acomplished with a bayesian approach or a planning based approach. Either way, I am confident an RL agent could play this game, but it might just not be the most interesting or useful application.</p>
]]></content></item><item><title>Is Reward Enough</title><link>tylerjamesmalloy.github.io/posts/is-reward-enough/</link><pubDate>Mon, 17 Jan 2022 00:00:00 +0000</pubDate><guid>tylerjamesmalloy.github.io/posts/is-reward-enough/</guid><description>In this post I provide a review and opinion on the paper &amp;ldquo;Reward Is Enough&amp;rdquo; by D Silver, S Singh, D Precup, and RS Sutton. In this work, the authors provide a broad perspective on reinforcement learning research and put forward the opinion that much of the behavior that interests cognitive science and artificial intelligence researchers can be viewed in relation to reward. Specifically, they propose that many cognitive faculties such as perception, language, generalization, imitation, and even general intelligence can be achieved through reward maximization and experience in an environment.</description><content type="html"><![CDATA[<p>In this post I provide a review and opinion on the paper <a href="https://www.sciencedirect.com/science/article/pii/S0004370221000862">&ldquo;Reward Is Enough&rdquo;</a> by D Silver, S Singh, D Precup, and RS Sutton. In this work, the authors provide a broad perspective on reinforcement learning research and put forward the opinion that much of the behavior that interests cognitive science and artificial intelligence researchers can be viewed in relation to reward. Specifically, they propose that many cognitive faculties such as perception, language, generalization, imitation, and even general intelligence can be achieved through reward maximization and experience in an environment. They describe a hypothesis alongside these claims that is essentially stated in the short title, that reward is enough to learn these types of complex behaviors. The following figure borrowed from the paper describes several phenomenon which could hypothetically be trained through reward based reinforcement style learning.</p>
<!-- raw HTML omitted -->
<p><!-- raw HTML omitted --><!-- raw HTML omitted --><!-- raw HTML omitted -->Fig.1 - Reward Is Enough
David Silver, Satinder Singh, Doina Precup, Richard S.Sutton (paper on ScienceDirect). This figure demonstrates the overlap in behaviour that can conceviably be taught through reward signals in a cognitive vs. artificial agent.<!-- raw HTML omitted --><!-- raw HTML omitted --></p>
<!-- raw HTML omitted -->
<p>For a cognitive psychologist or cognitive philosopher the first impression of this claim and the themes of the paper may be somewhat negative. Haven’t Chomsky and others taught us that experience in language use alone cannot give us the tools we need to be good language users? There should be a requirement that a universal grammar exist to reduce the hypothesis space of possible grammars before we even begin understanding the utterances of others, let alone generating our own. In point of fact there is a significant dearth of ‘negative’ examples of proper language, as most of our experience is with well formed language. Furthermore, much of our experience with language happens internally, without a clearly defined external reward signal.</p>
<p>Although these potential issues can be raised when taking the reward-is-enough hypothesis as a general claim or descriptive thesis on human cognition, in reality the goals of the paper are in showing that reward is sufficient for complex behavior learning. Because of this, the main purpose of the paper could be supported even without any evidence of reward based learning in a human agent. Instead the paper seeks to provide evidence that artificial reinforcement learning agents could hypothetically learn the complex behavior that humans achieve through reward signals alone.</p>
<p>While it is true that the paper claims to be more interested in describing the ‘sufficient’ aspect of learning behavior through reward, it does at the same time make some claims and connections to cognitive science that are more controversial. One source of potential controversy is the section entitled “What else, other than reward maximisation, could be enough for intelligence?” In this section the authors provide brief outlines of alternative hypotheses and suggest that they are not as well fit for training goals of general AI and other interesting behaviors. The presence of this section raises the question of the true intentions of the paper as a whole. If as the authors claim the hypothesis is centered around how behavior could be taught and not a description of human cognition, then why iterate through a list of alternatives and claim they cannot do what reward alone can?</p>
<p>In particular, the brief sentences on the free-energy hypothesis arguably leave out some important claims by cognitive psychologists such as Karl Friston in his paper “The free-energy principle: a unified brain theory?” The full quote from ‘Reward Is Enough’ is as follows: “Maximisation of free energy or minimisation of surprise may yield several abilities of natural intelligence, but does not provide a general-purpose intelligence that can be directed towards a broad diversity of different goals in different environments. Consequently it may also miss abilities that are demanded by the optimal achievement of any one of those goals (for example, aspects of social intelligence required to mate with a partner, or tactical intelligence required to checkmate an opponent).”</p>
<p>It seems from Friston’s perspective that free energy alone could provide much of what reward does for the authors of this paper. At least this should have more of a discussion if the authors are interested in showing why reward is unique in its ability. If they are not then it may be better to eschew a discussion of alternatives or claims that reward is specifically unique in its position. Otherwise, it is hard to not make larger connections of the claims made in the paper to a description of human cognition.</p>
<p>To me the general theme of the paper actually reminded me much of Karl Friston’s paper previously mentioned. There are many complex behaviors that could be defined as achieving something like free-energy minimization or reward maximization. Generally this is a good place to start for machine learning or artificial intelligence research because if you can begin to define what desirable behaviour looks like in terms that resemble something like a trainable loss function, then you are well on your way to making a useful system. This may be interesting from an AI researcher or engineering perspective, but for a cognitive psychologist the claims could be seen as somewhat vacuous and less useful for understanding cognition.</p>
<p>If you are interested in my perspective, I think that at least for much of human learning and decision making there is a combination of reward driven reinforcement style learning and predictive processing used for planning and beliefs about future states. However, I will add that I make no claims of how language and general intelligence specifically can and should be taught, as those are outside of the realm of my particular experience. I would imagine that much of those more complex behaviors is more driven by the structure of cognitive architectures as they have been optimized through millions of years of evolution.</p>
]]></content></item><item><title>Masters Thesis 2020</title><link>tylerjamesmalloy.github.io/posts/masters-thesis-2020/</link><pubDate>Tue, 28 Dec 2021 00:00:00 +0000</pubDate><guid>tylerjamesmalloy.github.io/posts/masters-thesis-2020/</guid><description>This post is another retrospective, but instead of a conference or journal paper it takes a look at my masters thesis, titled &amp;ldquo;Modelling Learning and Decision Making Under Information Processing Constraints&amp;rdquo;. This blog post will go through the begninning stages of the project and how it ultimately narrowed down the focus of the thesis and project into what it eventually became.
Very early on in my PhD, I became interested my advisor Chris Sims&amp;rsquo; previous work using infromation theory, specifically mutual information, as a tool to understand the cognitive costs of behaviour in learning and decision making tasks.</description><content type="html"><![CDATA[<p>This post is another retrospective, but instead of a conference or journal paper it takes a look at my masters thesis, titled &ldquo;Modelling Learning and Decision Making Under Information Processing Constraints&rdquo;. This blog post will go through the begninning stages of the project and how it ultimately narrowed down the focus of the thesis and project into what it eventually became.</p>
<p>Very early on in my PhD, I became interested my advisor Chris Sims&rsquo; previous work using infromation theory, specifically mutual information, as a tool to understand the cognitive costs of behaviour in learning and decision making tasks. While this may seem somewhat narrow, there are many possible applications of information theory in this way, and a wide range of psychological experimentation that has been done looking into modelling constrained cognition. We became interested in bandit learning tasks due to their simplicity, long history, and the publically available datasets with human participant responses. Since this was early on in my PhD and a while before I would eventually propose my own experiment to run, it made sense to test the ideas I had on another similar task that could guide my research in my PhD.</p>
<p>This inspired the project I worked on that would become an abstract paper in the Reinforcement Learning and Decision Making conference, and a slight extension of that paper is essentially the final section of my masters thesis. However, when I began the work in writing up my thesis, I realized I had become interested in the decision making setting during my economic modelling course. A large portion of this course was modelling decision making under risk and uncertainty, which is closely related to the decision making that takes place when learning in the bandit setting. I was wondering if the same concepts of mutual information and behavioural complexity could be used in this slightly different setting.</p>
<p>The key difference in the decision making setting is that the outcome utiltiies and probabilities that determine optimal behaviour are given directly, instead of being learned through experience as in the learning setting. This second interest and application ended up being roughly a third to a half of the content in my thesis, as it required a long background on decision making in various settings, which are some of the oldest and most well studied phenomenon in cognitive science. Taking a look back at my thesis, I realize it may seem like a lot of extra work to include only a slightly different phenomenon, but I am glad I did include it as it allowed me to relate and contrast my understanding of cognition with similar accounts from a wide variety of cognitive modells.</p>
<p>Broadly this experience in extending my ideas into related domains taught me that it is important to show as broad an application as possible, or at least as you find interesting. Often these models and approaches are applied to a very small domain which can make it difficult to relate to human cognition as we know it is extremely broad in its application. Additionally I was able to expand on my knowledge of cognitive modelling methods in similar domains as the ones I had experience in, which I am similarly greatful for.</p>
]]></content></item><item><title>AAAI 2021</title><link>tylerjamesmalloy.github.io/posts/aaai-2021/</link><pubDate>Sun, 26 Dec 2021 00:00:00 +0000</pubDate><guid>tylerjamesmalloy.github.io/posts/aaai-2021/</guid><description>This is the second post in a series of retrospectives on previous work I have done that shaped my PhD and are related to my future research goals. If you would like to read the paper you can find it on my Researchgate.
Although this paper is relatively recent, I thought I would use it as my second post because it is closely realted to a project that I began soon after beginning my AI Researcher position with IBM in early 2019.</description><content type="html"><![CDATA[<p>This is the second post in a series of retrospectives on previous work I have done that shaped my PhD and are related to my future research goals. If you would like to read the paper you can find it on my <a href="https://www.researchgate.net/publication/354551984_Capacity-Limited_Decentralized_Actor-Critic_for_Multi-Agent_Games">Researchgate</a>.</p>
<p>Although this paper is relatively recent, I thought I would use it as my second post because it is closely realted to a project that I began soon after beginning my AI Researcher position with IBM in early 2019. I actually began this project before I presented the previous project at RLDM 2019. This earlier project, which eventually became an <a href="https://www.researchgate.net/publication/349345269_Consolidation_via_Policy_Information_Regularization_in_Deep_RL_for_Multi-Agent_Games">arxiv paper</a>, was influential in my interest in robotics, machine learning, and applications of reinforcement learning.</p>
<p>After this earlier project applying reinforcement learning to a robotics simulation environment, I extended the general idea into the project and paper that goes along with it &ldquo;Capacity-Limited Decentralized Actor-Critic for Multi-Agent Games&rdquo;. This was a fortunate paper as it us to have a published work on mutual information regularized reinforcement learning. This was the main idea around the earlier metod applied to robotics. Essentially the main idea is that we want to make the behaviour learned by robots as informationally simplistic as possible. This is inspired by human behaviour which we believe strives to be similarly informationally simplistic where possible.</p>
<p>This main idea was applied into a set of complex multi-agent environments with agents using a continuous control paradigm, as well as a degree of communication in some environments. We showed in our experimentation that applying a penalty to agent behavioural complexity at the learning stage can improve learning speed as well as the eventual highest performance of the agent. Compared to agents without this complexity constraint, our agents learned more generalizable behaviour that was simultaneously less informationally complex.</p>
<p>This was a great result for our ideas of human-inspired reinforcement leanring, and I will probably make a seperate blog post in the future explaining those ideas in the future. Since this project, our ideas of human-inspired RL have slightly shifted, but this was an important first step. Recently in our work applying lessons from how humans learn complex tasks onto reinforcement learning we have focused this &lsquo;information constraint&rsquo; view in different ways, but in spirit this is the same concept as these earlier papers.</p>
]]></content></item><item><title>Predicting Human Choice</title><link>tylerjamesmalloy.github.io/posts/predicting-human-choice/</link><pubDate>Wed, 15 Dec 2021 00:00:00 +0000</pubDate><guid>tylerjamesmalloy.github.io/posts/predicting-human-choice/</guid><description>This is the first post in a series of retrospectives looking back at papers and conferences I have attended. Now that I am entering my final year of my PhD, I will be begining this as a chronicle of projects, papers and conferences that influenced my time during my PhD.
This was my first accepted paper, I submitted it to the conference Reinforcement Learning and Decision Making in 2019. I attended the conference which was a great experience.</description><content type="html"><![CDATA[<p>This is the first post in a series of retrospectives looking back at papers and conferences I have attended. Now that I am entering my final year of my PhD, I will be begining this as a chronicle of projects, papers and conferences that influenced my time during my PhD.</p>
<p>This was my first accepted paper, I submitted it to the conference Reinforcement Learning and Decision Making in 2019. I attended the conference which was a great experience. I didn&rsquo;t know then but it would be one of the few in-person conferences I would attend! Although many of the online conferences I had been to since have been great experiences, the value of in-person poster and talk presentations can&rsquo;t be understated. I hope to go back to in-person conferences before the end of my PhD.</p>
<p>Now onto the contents of this paper. Looking back at it, I didn&rsquo;t know it then but this would be a big impact on the direction of my research moving forward. This paper took a data set that had generously been made publically available by the <a href="https://nivlab.princeton.edu/">Niv Lab</a> at Princeton. Since it was my first year of my PhD, I didn&rsquo;t have the direction yet to design an experiment with human participants, and this helped significantly. The motivation of my paper was looking into modelling human learning using more modern approaches that had recently been gaining attention in reinforcement learning research.</p>
<p>The specific method I looked into was actor-critic reinforcement learning, which had been used in both artificial intelligence research as well as some cognitive modelling approaches. The difference in predictive accruacy was modest, but from my perspective the paper serves to support the body of work that uses more complex and modern reinforcement learning and artificial intelligence methods for cognitive modelling. This contribution was the focus of my paper writing and my discussions during my poster presentation.</p>
<p>Since this conference, I have continued to use this dataset in testing the model I am currently working with on my PhD thesis. Additionally, the design of the learning task inspired in part the experiment that I will soon be running. It&rsquo;s nice to look back at this early conference paper and think about how it has shaped my research moving forward.</p>
<p>Check out the <a href="https://www.researchgate.net/publication/335663034_Predicting_Human_Choice_in_a_Multi-Dimensional_N-Armed_Bandit_Task_Using_Actor-Critic_Feature_Reinforcement_Learning">paper on my researchgate profile here</a> if you are interested.</p>
]]></content></item><item><title>New Year 2022</title><link>tylerjamesmalloy.github.io/posts/new-year-2022/</link><pubDate>Sun, 03 Jan 2021 00:00:00 +0000</pubDate><guid>tylerjamesmalloy.github.io/posts/new-year-2022/</guid><description>Rather than look back at previous research I have done, as the previous posts on this blog have done, this post will look forward to my hopes for 2022 and new research ideas I am interested in. Firstly, the major plans for this year include completing the website hosting my thesis project, submitting a paper based on my work in Theory of Mind for reinforcement learning, and completing my PhD Thesis.</description><content type="html"><![CDATA[<p>Rather than look back at previous research I have done, as the previous posts on this blog have done, this post will look forward to my hopes for 2022 and new research ideas I am interested in. Firstly, the major plans for this year include completing the website hosting my thesis project, submitting a paper based on my work in Theory of Mind for reinforcement learning, and completing my PhD Thesis. Aside from that, on a more personal level I will be taking some time this year to look for possible post-doc positions or research centric positions in other areas. As a part of that I hope to continue with this blog and additionally go back to some of my previous posts and add a bit of information. Additionally I hope to expand on some of my background knowledge of other areas of cognitive science, psychology, and machine learning.</p>
<p>That all sounds like a lot of things! So this year is looking to be a busy one for me. However, I think that a lot of these goals are both interesting and necessary for my future plans, so I will try to take that motivation forward in pursuing these goals. Generally I am not one for clearly defined &rsquo;new years resolutions&rsquo;, because I try to improve myself at a slow and stedy pace, to increase the chance of making lasting changes and achieving long term goals. But I do think that it is good to take the time at the beginning of the year or even just the changing of the seasons to reflect on aspirations, goals, and so on. And there is no time like the last year of a PhD to reflect on these things!</p>
<p>Concretely, the priorities I have are focused around the completion of my PhD since I hope to do that with as few complications as possible, and my position currently affords a good opertunity for that. So my main focuses are towards the research most central to my PhD, as well as things that will be useful for my future as a researcher including relevant research for potential postdoc or other positions. Those specific goals definitely seem relatively focused, and I can always add on top of that some of the other ideas I have depending on whether I have the time for them.</p>
<p>Some of the personal aspirations I have other than research and job prospect related things are to read more fiction, as I do end up spending a good amount of time reading books and papers related to my research interests, and I&rsquo;d like to get back into my hobby of science-fiction and fantasy. I did get a great christmas gift of two books by JRR Tolkien that I am excited to read, and I am also in the middle of a fantastic high-fantasy book called &ldquo;The Priory of the Orange Tree&rdquo; by Samantha Shannon. I hope to read those and other books this year, as I have been enjoying slowly getting back into reading for fun. I think it is useful to put down my thoughts and aspirations in this blog as it allows me to reflect on them more than simply thinking about them to myself.</p>
]]></content></item></channel></rss>