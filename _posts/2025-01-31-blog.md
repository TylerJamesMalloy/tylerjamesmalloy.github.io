---
title: "Programming Computational Cognitive Models with GPT-4o"
date: 2025-01-31
draft: false 
toc: false
layout: post
permalink: /posts/2024-11-16
author_profile: false
tags:
  - Cognitive Modelling
  - Artificial Intelligence
  - Large Language Models
---

## Web Searching Abilities 

One of the most exciting features of modern LLMs is their ability to actively search through the internet and look for information that might be related to a prompt it is trying to respond to. I have used GPT while coding many times when I forget the name of a function in seaborn, pandas, or numpy, and need to quickly get the name without opening google, typing the question, looking through stack overflow, seeing that the question is slightly different, going back, and clicking on another stack overflow link. 

The ability to search through python libraries and find code snippets that are relevant to a specific task is fantastic. And GPT-4o allows you to upload files for it to search through and reference to answer even more specific questions you may have about your code. In this blog post I will detail how Chat-GPT4o can be used as a tool for programming computational cognitive models. 

<p align="center">
  <img src="https://www.tylermalloy.com/assets/images/gpt4o.png" />
</p>

## The PyIBL python library 

[PyIBL](https://ddm-lab.github.io/pyibl-documentation/) is a library for python that implements some of the functions and methods used in creating IBL cognitive models. There is a fantastic introduction to making cognitive model agents with PyIBL such as the example of how to create a [binary choice cognive model](https://ddm-lab.github.io/pyibl-documentation/binary-choice.html) among many others. The introduction and examples are very thourough and were written by members of the DDM lab including Professor Cleotilde Gonzalez and Don Morrison. Luckily, these examples are included in the github repository for the [PyIBL library](https://github.com/DDM-Lab/pyibl). Another library that PyIBL draws off of is PyACTUP which implements some functions of the ACT-R cognitive architecture. Let's make a new project with the pyibl and pyactup files in them and see what we can do by putting them into Chat-GPT-4o. Using GPT-4o directly this way allows you to upload files one at at time, but that is alright as the pyactup and pyibl libraries are themselves only one python file. I uploaded these two files along with some example files and asked GPT-4o to tell me about them. 

<p align="center">
  <img src="https://www.tylermalloy.com/assets/images/tellMeAbout.png" />
</p>

`
Can you make a IBL agent model that can make selections between three choices that each have three features (shape, color, and texture), each of which can have three attributes (square, circle, triangle), (red, blue, green) and (wavy, dotted, hatched). Make the simulation randomize each of those attributes and always include all 9 in the choices. 
`
```python
import random
import matplotlib.pyplot as plt
from pyibl import Agent

# Define possible attributes for each feature
SHAPES = ["square", "circle", "triangle"]
COLORS = ["red", "blue", "green"]
TEXTURES = ["wavy", "dotted", "hatched"]

# Number of rounds the agent will play
ROUNDS = 100

# Initialize IBL agent
agent = Agent(attributes=["shape", "color", "texture"], default_utility=1.0)

def generate_choices():
    """Generate 3 choices, each with a unique combination of shape, color, and texture."""
    choices = []
    used_combinations = set()
    while len(choices) < 3:
        shape = random.choice(SHAPES)
        color = random.choice(COLORS)
        texture = random.choice(TEXTURES)
        combo = (shape, color, texture)
        if combo not in used_combinations:
            used_combinations.add(combo)
            choices.append(combo)
    return choices

def assign_reward(choice):
    """Assign a reward based on a simple heuristic."""
    return (SHAPES.index(choice[0]) + 1) + (COLORS.index(choice[1]) + 1) + (TEXTURES.index(choice[2]) + 1)

results = []
for _ in range(ROUNDS):
    choices = generate_choices()
    selected_choice = agent.choose(choices)
    reward = assign_reward(selected_choice)
    agent.respond(reward)
    results.append((selected_choice, reward))

# Analyze results
choices_made = [choice for choice, _ in results]
rewards_received = [reward for _, reward in results]

# Plot reward trend over time
plt.plot(range(ROUNDS), rewards_received, label="Reward per Round")
plt.xlabel("Round")
plt.ylabel("Reward")
plt.title("IBL Agent Decision Learning Over Time")
plt.legend()
plt.show()

### Display final learned choices
print("Final Choices Made by the Agent:")
for i, (choice, reward) in enumerate(results[-10:]):
    print(f"Round {ROUNDS - 10 + i}: {choice} -> Reward: {reward}")
```

`
Use pandas to keep track of the reward using a data frame that stores the stimuli, action taken, and reward observed.  Plot this pandas data frame at the end. Use 50 agents and make the round length longer. 
`

<p align="center">
  <img src="https://www.tylermalloy.com/assets/images/FirstTry.png" />
</p>

One interesting thing is that instead of outputting a fully new python script, Chat GPT-4o will edit the file that it just made and allow you to edit the code yourself if you want. Some of the requests in my prompt would have been easy to implement. 

```python
import random
import matplotlib.pyplot as plt
import pandas as pd
from pyibl import Agent

### Define possible attributes for each feature
SHAPES = ["square", "circle", "triangle"]
COLORS = ["red", "blue", "green"]
TEXTURES = ["wavy", "dotted", "hatched"]

### Number of rounds and agents
ROUNDS = 200
AGENTS = 50

### Initialize agents
agents = [Agent(attributes=["shape", "color", "texture"], default_utility=1.0) for _ in range(AGENTS)]

def generate_choices():
    """Generate 3 choices, each with a unique combination of shape, color, and texture."""
    choices = []
    used_combinations = set()
    while len(choices) < 3:
        shape = random.choice(SHAPES)
        color = random.choice(COLORS)
        texture = random.choice(TEXTURES)
        combo = (shape, color, texture)
        if combo not in used_combinations:
            used_combinations.add(combo)
            choices.append(combo)
    return choices

def assign_reward(choice):
    """Assign a reward based on a simple heuristic."""
    return (SHAPES.index(choice[0]) + 1) + (COLORS.index(choice[1]) + 1) + (TEXTURES.index(choice[2]) + 1)

# Data collection
data = []
for agent_id, agent in enumerate(agents):
    for round_num in range(ROUNDS):
        choices = generate_choices()
        selected_choice = agent.choose(choices)
        reward = assign_reward(selected_choice)
        agent.respond(reward)
        data.append({
            "Agent": agent_id,
            "Round": round_num,
            "Choices": choices,
            "Action Taken": selected_choice,
            "Reward Observed": reward
        })

# Convert to pandas DataFrame
df = pd.DataFrame(data)

# Plot reward trend over time
reward_means = df.groupby("Round")["Reward Observed"].mean()
plt.plot(reward_means.index, reward_means.values, label="Average Reward per Round")
plt.xlabel("Round")
plt.ylabel("Average Reward")
plt.title("IBL Agent Decision Learning Over Time")
plt.legend()
plt.show()

# Display final DataFrame
import ace_tools as tools
tools.display_dataframe_to_user(name="IBL Agent Results", dataframe=df)
```


<p align="center">
  <img src="https://www.tylermalloy.com/assets/images/SecondTry.png" />
</p>

For some reason the script was importing something called ace_tools, which I haven't heard of, at the end of the script. This made it impossible to run the python script in the command line in the chatGPT application. Once I removed that I recieved an output. 

From this output we can see that the IBL model is at least learning with 200 rounds, but that isn't quite as fast as what we might expect from a human performing this task. The reward function that GPT-4o made is a bit simplistic

`
def assign_reward(choice):
    """Assign a reward based on a simple heuristic."""
    return (SHAPES.index(choice[0]) + 1) + (COLORS.index(choice[1]) + 1) + (TEXTURES.index(choice[2]) + 1)

`

So let's see if GPT can edit the parameters of the IBL model in a way that can improve the speed of learning.

`
Edit the parameters of the IBL model to improve the training speed.
`

```python
import random
import matplotlib.pyplot as plt
import pandas as pd
from pyibl import Agent

# Define possible attributes for each feature
SHAPES = ["square", "circle", "triangle"]
COLORS = ["red", "blue", "green"]
TEXTURES = ["wavy", "dotted", "hatched"]

# Number of rounds and agents
ROUNDS = 200
AGENTS = 50

# Initialize agents with optimized parameters
agents = [
    Agent(
        attributes=["shape", "color", "texture"], 
        default_utility=1.0, 
        noise=0.1,  # Reduce randomness in decision-making
        decay=0.3,  # Moderate decay for faster convergence
        temperature=0.5,  # Controls the influence of rewards
        optimized_learning=True  # Enables optimized learning for speed
    ) for _ in range(AGENTS)
]

def generate_choices():
    """Generate 3 choices, each with a unique combination of shape, color, and texture."""
    choices = []
    used_combinations = set()
    while len(choices) < 3:
        shape = random.choice(SHAPES)
        color = random.choice(COLORS)
        texture = random.choice(TEXTURES)
        combo = (shape, color, texture)
        if combo not in used_combinations:
            used_combinations.add(combo)
            choices.append(combo)
    return choices

def assign_reward(choice):
    """Assign a reward based on a simple heuristic."""
    return (SHAPES.index(choice[0]) + 1) + (COLORS.index(choice[1]) + 1) + (TEXTURES.index(choice[2]) + 1)

# Data collection
data = []
for agent_id, agent in enumerate(agents):
    for round_num in range(ROUNDS):
        choices = generate_choices()
        selected_choice = agent.choose(choices)
        reward = assign_reward(selected_choice)
        agent.respond(reward)
        data.append({
            "Agent": agent_id,
            "Round": round_num,
            "Choices": choices,
            "Action Taken": selected_choice,
            "Reward Observed": reward
        })

# Convert to pandas DataFrame
df = pd.DataFrame(data)

# Plot reward trend over time
reward_means = df.groupby("Round")["Reward Observed"].mean()
plt.plot(reward_means.index, reward_means.values, label="Average Reward per Round")
plt.xlabel("Round")
plt.ylabel("Average Reward")
plt.title("IBL Agent Decision Learning Over Time")
plt.legend()
plt.show()
```

Here we can see that the learning is almost exactly the same. Let's try giving GPT some help by suggesting the model prepopulate instances into the IBL agent, which would speed up it's learning. 

`
Try speeding up the learning by using pre populated instances in memory of the rewards associated with each feature. 
`

This made GPT-4o add the following lines to the code:

`
# Prepopulate memory with feature-based rewards
def prepopulate_memory(agent):
    for shape in SHAPES:
        for color in COLORS:
            for texture in TEXTURES:
                reward = (SHAPES.index(shape) + 1) + (COLORS.index(color) + 1) + (TEXTURES.index(texture) + 1)
                agent.populate([(shape, color, texture)], reward)
`

<p align="center">
  <img src="https://www.tylermalloy.com/assets/images/ThirdTry.png" />
</p>


This produces behavior that consistently selects the best option which on average is about 7.5, the top value is 9 but this only happens when the three high value attributes are there at the same time. This isn't very human like behavior and is moreso acting like someone who already knows all the values, but given that I didn't instruct GPT-4 to make the learning human-like while speeding it up, it makes sense that it would try putting all shape-color-texture combinations in memory. 

This was an interesting result, next time I am going to try to get GPT-4o to make a model that can perform one of the tasks that I recently submitted to review, a delayed feedback condition. 