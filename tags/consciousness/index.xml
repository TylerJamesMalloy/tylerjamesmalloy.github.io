<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Consciousness on Human and Machine Learning in Cognitive Science</title><link>tylerjamesmalloy.github.io/tags/consciousness/</link><description>Recent content in Consciousness on Human and Machine Learning in Cognitive Science</description><generator>Hugo</generator><language>en</language><copyright>&lt;a href="https://creativecommons.org/licenses/by-nc/4.0/" target="_blank" rel="noopener">CC BY-NC 4.0&lt;/a></copyright><lastBuildDate>Tue, 14 Feb 2023 00:00:00 +0000</lastBuildDate><atom:link href="tylerjamesmalloy.github.io/tags/consciousness/index.xml" rel="self" type="application/rss+xml"/><item><title>Narrowing The Gap With LLMs</title><link>tylerjamesmalloy.github.io/posts/narrowing-gap/</link><pubDate>Tue, 14 Feb 2023 00:00:00 +0000</pubDate><guid>tylerjamesmalloy.github.io/posts/narrowing-gap/</guid><description>This will be my third blog post in a row (see the first, and second) on the topic of large language models. While this area of research has been in the news significantly recently, it is not exactly my area. However, there was a talk at the University of Pittsburgh philosophy department given by professor Colin Allen that was, at least partially, presented as a refutation to the position of David Chalmers that I discussed previously.</description></item></channel></rss>