<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>large language models on Human and Machine Learning in Cognitive Science</title><link>tylerjamesmalloy.github.io/tags/large-language-models/</link><description>Recent content in large language models on Human and Machine Learning in Cognitive Science</description><generator>Hugo -- gohugo.io</generator><language>en</language><copyright>&lt;a href="https://creativecommons.org/licenses/by-nc/4.0/" target="_blank" rel="noopener">CC BY-NC 4.0&lt;/a></copyright><lastBuildDate>Sun, 29 Oct 2023 00:00:00 +0000</lastBuildDate><atom:link href="tylerjamesmalloy.github.io/tags/large-language-models/index.xml" rel="self" type="application/rss+xml"/><item><title>AAAI Symposium on Generative Models in Cognitive Architectures</title><link>tylerjamesmalloy.github.io/posts/fss-2023/</link><pubDate>Sun, 29 Oct 2023 00:00:00 +0000</pubDate><guid>tylerjamesmalloy.github.io/posts/fss-2023/</guid><description>AAAI Fall Symposium Series - Integration of Generative Models in Cognitive Architectures Introduction (John Laird) Introduction to Structure and Themes of Symposium John Laird Cognitive architectures need help from generative models. Multiple representations of knowledge are relevant for the design and structure of cognitive architectures. &amp;lsquo;Best match to brain structure for dynamic resting brain imaging; LLMs: Transformers interesting because they can determine which features of the prompt are relevant for predicting the next words.</description></item><item><title>Applying LLMs in Cognitive Models</title><link>tylerjamesmalloy.github.io/posts/aaai-2023/</link><pubDate>Tue, 29 Aug 2023 00:00:00 +0000</pubDate><guid>tylerjamesmalloy.github.io/posts/aaai-2023/</guid><description>Two submissions our lab has been working on were recently accepted to the AAAI 2023 Fall Symposium Series on the Integration of Cognitive Architectures and Generative Models. This was an exciting series for me to see since I have written multiple blog posts on this site about LLMs, but hadn&amp;rsquo;t thought deeply on how to apply them to cognitive models. LLMs definitely have been an interest of mine, but I hadn&amp;rsquo;t had a good reason to push for their integration in the models that we work with.</description></item><item><title>Job Security</title><link>tylerjamesmalloy.github.io/posts/job-security/</link><pubDate>Wed, 05 Apr 2023 00:00:00 +0000</pubDate><guid>tylerjamesmalloy.github.io/posts/job-security/</guid><description>I recently completed the last meeting with a small group of mentees from my alma mater UBC that was put together to give undergraduate students at different points in their career some insight into life after undergrad. I think it was a great experience for myself, as I had little previous work as a direct mentor, and hopefully the mentees felt the same way. The remainder of this blog post is some reflections from my experience as a mentor and the life lessons I have thought of and tried to discuss with them.</description></item><item><title>Narrowing The Gap With LLMs</title><link>tylerjamesmalloy.github.io/posts/narrowing-gap/</link><pubDate>Tue, 14 Feb 2023 00:00:00 +0000</pubDate><guid>tylerjamesmalloy.github.io/posts/narrowing-gap/</guid><description>This will be my third blog post in a row (see the first, and second) on the topic of large language models. While this area of research has been in the news significantly recently, it is not exactly my area. However, there was a talk at the University of Pittsburgh philosophy department given by professor Colin Allen that was, at least partially, presented as a refutation to the position of David Chalmers that I discussed previously.</description></item></channel></rss>