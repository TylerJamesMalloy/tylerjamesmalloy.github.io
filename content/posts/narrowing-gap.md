---
title: "Narrowing The Gap With LLMs: Colin Allen"
date: 2023-02-14
draft: false
toc: false
images:
tags:
  - presentation
  - large language models
  - consciousness 
---

This will be my third blog post in a row (see the [first](https://tylerjamesmalloy.github.io/posts/sentience/), and [second](https://tylerjamesmalloy.github.io/posts/neurips-2022/)). While this area of research has been in the news signifcantly recently, it is not exatly my area. However, there was a talk at the University of Pittsburgh philosophy department given by professor Colin Allen that was, at least partially, presented as a refutation to the position of David Chalmers that I discussed previously. Since I talked about Chalmers' ideas in my previous blog post, I thought this would serve as a nice contrast to those ideas, and hopefully wrap up my thinking about it for the time being. 

In Dr. Allen's talk, he discusses some issues with large language models and the continuing gap between AI and human intelligence. One of the arguments that Allen raises about the current issues with LLMs are the level to which they display 'thruthiness', a term coined by the late night comedian Stephen Colbert to refer to plausible sounding nonsense. This phenomenon can be seen on many examples of twitter posts and blog articles about the failures of chatGPT and related LLMs. In the talk, Allen presented his own exmaple where he repeatedly told chatGPT that its response was factually incorrect, which chatGPT agreed with, however it could not provide the correct response. 

This over-agreeable behaviour is something I have noticed when using these LLM systems, that they rarely dsiplay their own preferences and refuse to take stances on many issues or opinions. Also, they often change their mind when presented with different points of view such as an argument I recently got into with chatGPT about the differences and similarities of the film Bladerunner and the book Do Android's Dream of Electric Sheep. During that discussion, chatGPT repeatedly mentioned that the movie is more 'visual'. When I said that that distinciton isn't relevant when comparing a book and movie, chatGPT would initially agree with me before mentioning the same point again. 

Dr Allen at one point summarized the position of David Chalmer's argument that LLMs, thought not currently sentient, could be added to in specific ways that would increase the liklihood that they would display some level of sentience. Allen diagreed with this general notion by arguing that what is truly missing from LLMs, that they can't solve with more layers or training, is an architecture that allows for meta-cognition. It is important, to Allen, that conscious agent's be able to 'take a step back' and see why they are incorrect or correct using reasoning and logic. Currently logic problems and mathematics are big issues for vanilla LLMs, though they can be somewhat overcome by prompt engineering. 

Personally, I agree in the importance of a meta-cognitive architecture that allows models to use different faculties to address a problem, and oversee the reasoning used to decide what faculty to use. However, the behaviour of LLMs have already demonstrated some phenomenon that philosophy of AI researchers previously have thought would require a specific architecture and training method, rather than the relatively simple end to end structure and training of LLMs. In Chalmer's talk, he noted the possibility that the easest way to train a model to do next in sentence word prediction may be to have the model learn a world representation that emerges through experience, an idea that was supported through recent studies of world representations in LLMs, mentioned in my last post. This could mean that the type of meta-cognitive architecture that Dr Allen argues for could theoretically emerge naturally though specific training methods and larger models. However, the end result may be further and further from how meta-cognition and logci is implemented in the human brain. 

Ultimately, I am unsure whether or not is is possible for meta-cognition, logic, compositionality, or even consciousness to emerge from the type of training and structure that current LLMs are designed on. I think that this will be an important question in the next twenty years of AI research, and it is important not to dismiss out of hand arguments in either direction. This is an exciting area of potential research that will need to bring together philosophers of the mind and AI with engineers and companies that are interested in making their products as human friendly as possible. 

